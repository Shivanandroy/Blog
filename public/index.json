[{"categories":["Search Engine"],"content":"Abstract\r\rThis article will let you build a faster and accurate COVID Search Engine using Transformersü§ó\r\r ","date":"06/09/2020","objectID":"/building-a-faster-accurate-covid-search-engine-with-transformers/:0:0","tags":["Deep Learning","Transformers","Search Engine"],"title":"Building A Faster \u0026 Accurate COVID Search Engine with Transformersü§ó","uri":"/building-a-faster-accurate-covid-search-engine-with-transformers/"},{"categories":["Search Engine"],"content":"Introduction In this article, we will build a search engine, which will not only retrieve and rank the articles based on the query but also give us the response, along with a 1000 words context around the response. To achieve this, we will need: a structured dataset with reserach papers and its full text. Transformers library to build QA model and Finally, Haystack library to scale QA model to thousands of documents and build a search engine. ","date":"06/09/2020","objectID":"/building-a-faster-accurate-covid-search-engine-with-transformers/:0:1","tags":["Deep Learning","Transformers","Search Engine"],"title":"Building A Faster \u0026 Accurate COVID Search Engine with Transformersü§ó","uri":"/building-a-faster-accurate-covid-search-engine-with-transformers/"},{"categories":["Search Engine"],"content":"Data For this tutorial, we will use COVID-19 Open Research Dataset Challenge (CORD-19). CORD-19 is a resource of over 200,000 scholarly articles, including over 100,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. This dataset is ideal for building document retrieval system as it has full research paper content in text format. Columns like paper_id: Unique identifier of research paper title: title of research paper abstract: Bried summary of the research paper full_text: Full text/content of the research paper are of our interest. In Kaggle Folder Structure - There are 2 directories - pmc_json and pdf_json - which contains the data in the json format. We will take 25,000 articles from pmc_json directory and 25000 articles from pdf_json - So, a total of 50,000 research articles to build our search engine. We will extract paper_id, title, abstract, full_text and put it in an easy to use pandas.DataFrame. ","date":"06/09/2020","objectID":"/building-a-faster-accurate-covid-search-engine-with-transformers/:0:2","tags":["Deep Learning","Transformers","Search Engine"],"title":"Building A Faster \u0026 Accurate COVID Search Engine with Transformersü§ó","uri":"/building-a-faster-accurate-covid-search-engine-with-transformers/"},{"categories":["Search Engine"],"content":"Code Note\r\rWe will use Kaggle notebook to write our code to leverage GPU.\r\r Load the data import numpy as np import pandas as pd import os import json import re from tqdm import tqdm dirs=[\"pmc_json\",\"pdf_json\"] docs=[] counts=0 for d in dirs: print(d) counts = 0 for file in tqdm(os.listdir(f\"../input/CORD-19-research-challenge/document_parses/{d}\")):#What is an f string? file_path = f\"../input/CORD-19-research-challenge/document_parses/{d}/{file}\" j = json.load(open(file_path,\"rb\")) #Taking last 7 characters. it removes the 'PMC' appended to the beginning #also paperid in pdf_json are guids and hard to plot in the graphs hence the substring paper_id = j['paper_id'] paper_id = paper_id[-7:] title = j['metadata']['title'] try:#sometimes there are no abstracts abstract = j['abstract'][0]['text'] except: abstract = \"\" full_text = \"\" bib_entries = [] for text in j['body_text']: full_text += text['text'] docs.append([paper_id, title, abstract, full_text]) #comment this below block if you want to consider all files #comment block start counts = counts + 1 if(counts \u003e= 25000): break #comment block end df=pd.DataFrame(docs,columns=['paper_id','title','abstract','full_text']) print(df.shape) df.head() pmc_json 34%|‚ñà‚ñà‚ñà‚ñé | 24999/74137 [01:29\u003c02:56, 278.06it/s] pdf_json 25%|‚ñà‚ñà‚ñç | 24999/100423 [01:24\u003c04:15, 295.09it/s] (50000, 4) We have 50,000 articles and columns like paper_id, title, abstract and full_text We will be interested in title and full_text columns as these columns will be used to build the engine. Let‚Äôs setup a Search Engine on top full_text - which contains the full content of the research papers. ","date":"06/09/2020","objectID":"/building-a-faster-accurate-covid-search-engine-with-transformers/:0:3","tags":["Deep Learning","Transformers","Search Engine"],"title":"Building A Faster \u0026 Accurate COVID Search Engine with Transformersü§ó","uri":"/building-a-faster-accurate-covid-search-engine-with-transformers/"},{"categories":["Search Engine"],"content":"Haystack Now, Welcome Haystack! The secret sauce behind setting up a search engine and ability to scale any QA model to thousands of documents. Haystack helps you scale QA models to large collections of documents! You can read more about this amazing library here https://github.com/deepset-ai/haystack For installation: ! pip install git+https://github.com/deepset-ai/haystack.git But just to give a background, there are 3 major components to Haystack. Document Store: Database storing the documents for our search. We recommend Elasticsearch, but have also more light-weight options for fast prototyping (SQL or In-Memory). Retriever: Fast, simple algorithm that identifies candidate passages from a large collection of documents. Algorithms include TF-IDF or BM25, custom Elasticsearch queries, and embedding-based approaches. The Retriever helps to narrow down the scope for Reader to smaller units of text where a given question could be answered. Reader: Powerful neural model that reads through texts in detail to find an answer. Use diverse models like BERT, RoBERTa or XLNet trained via FARM or Transformers on SQuAD like tasks. The Reader takes multiple passages of text as input and returns top-n answers with corresponding confidence scores. You can just load a pretrained model from Hugging Face‚Äôs model hub or fine-tune it to your own domain data. And then there is Finder which glues together a Reader and a Retriever as a pipeline to provide an easy-to-use question answering interface. Now, we can setup Haystack in 3 steps: Install haystack and import its required modules Setup DocumentStore Setup Retriever, Reader and Finder 1. Install haystack Let‚Äôs install haystack and import all the required modules # installing haystack ! pip install git+https://github.com/deepset-ai/haystack.git # importing necessary dependencies from haystack import Finder from haystack.indexing.cleaning import clean_wiki_text from haystack.indexing.utils import convert_files_to_dicts, fetch_archive_from_http from haystack.reader.farm import FARMReader from haystack.reader.transformers import TransformersReader from haystack.utils import print_answers 2. Setting up DocumentStore Haystack finds answers to queries within the documents stored in a DocumentStore. The current implementations of DocumentStore include ElasticsearchDocumentStore, SQLDocumentStore, and InMemoryDocumentStore. But they recommend ElasticsearchDocumentStore because as it comes preloaded with features like full-text queries, BM25 retrieval, and vector storage for text embeddings. So - Let‚Äôs set up a ElasticsearchDocumentStore. ! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q ! tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz ! chown -R daemon:daemon elasticsearch-7.6.2 import os from subprocess import Popen, PIPE, STDOUT es_server = Popen(['elasticsearch-7.6.2/bin/elasticsearch'], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1) # as daemon ) # wait until ES has started ! sleep 30 # initiating ElasticSearch from haystack.database.elasticsearch import ElasticsearchDocumentStore document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\") Once ElasticsearchDocumentStore is setup, we will write our documents/texts to the DocumentStore. Writing documents to ElasticsearchDocumentStore requires a format - List of dictionaries as shown below: [ {\"name\": \"\u003csome-document-name\u003e, \"text\": \"\u003cthe-actual-text\u003e\"}, {\"name\": \"\u003csome-document-name\u003e, \"text\": \"\u003cthe-actual-text\u003e\"} {\"name\": \"\u003csome-document-name\u003e, \"text\": \"\u003cthe-actual-text\u003e\"} ] (Optionally: you can also add more key-value-pairs here, that will be indexed as fields in Elasticsearch and can be accessed later for filtering or shown in the responses of the Finder) We will use title column to pass as name and full_text column to pass as the text # Now, let's write the dicts containing documents to our DB. document_store.write_documents(data[['ti","date":"06/09/2020","objectID":"/building-a-faster-accurate-covid-search-engine-with-transformers/:0:4","tags":["Deep Learning","Transformers","Search Engine"],"title":"Building A Faster \u0026 Accurate COVID Search Engine with Transformersü§ó","uri":"/building-a-faster-accurate-covid-search-engine-with-transformers/"},{"categories":["Search Engine"],"content":"ü•≥ Voila! We‚Äôre Done. Let‚Äôs see, how well our search engine works! - For simplicity, we will keep the number of documents to be retrieved to 2 using top_k_reader parameter. But we can extend to any number in production. Now, whenever we search or query our DocumentStore, we get 3 responses- we get the answer a 1000 words context around the answer and the name/title of the research paper Example 1: What is the impact of coronavirus on babies? question = \"What is the impact of coronavirus on babies?\" number_of_answers_to_fetch = 2 prediction = finder.get_answers(question=question, top_k_retriever=10, top_k_reader=number_of_answers_to_fetch) print(f\"Question: {prediction['question']}\") print(\"\\n\") for i in range(number_of_answers_to_fetch): print(f\"#{i+1}\") print(f\"Answer: {prediction['answers'][i]['answer']}\") print(f\"Research Paper: {prediction['answers'][i]['meta']['name']}\") print(f\"Context: {prediction['answers'][i]['context']}\") print('\\n\\n') Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.17 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.83 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.71 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.75 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.78 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.83 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01\u003c00:00, 1.08s/ Batches] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01\u003c00:00, 1.09s/ Batches] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.83 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.57 Batches/s] Question: What is the impact of coronavirus on babies? #1 Answer: While babies have been infected, the naivete of the neonatal immune system in relation to the inflammatory response would appear to be protective, with further inflammatory responses achieved with the consumption of human milk. Research Paper: COVID 19 in babies: Knowledge for neonatal care Context: ance to minimize the health systems impact of this pandemic across the lifespan.The Covid-19 pandemic has presented neonatal nurses and midwives with challenges when caring for mother and babies. This review has presented what is currently known aboutCovid-19 and neonatal health, and information and research as they are generated will add to a complete picture of the health outcomes. While babies have been infected, the naivete of the neonatal immune system in relation to the inflammatory response would appear to be protective, with further inflammatory responses achieved with the consumption of human milk. The WHO has made clear recommendations about the benefits of breastfeeding, even if the mother and baby dyad is Covid-19 positive, if they remain well. The mother and baby should not be separated, and the mother needs to be able to participate in her baby's care and develop her mothering role. The complexities of not being able to access her usual support people mean that the mother #2 Answer: neonate are mild, with low-grade fever and gastrointestinal signs such as poor feeding and vomiting. The respiratory symptoms are also limited to mild tachypnoea and/or tachycardia. Research Paper: COVID 19 in babies: Knowledge for neonatal care Context: Likewise, if the mother and baby are well, skin-to-skin and breast feeding should be encouraged, as the benefits outweigh any potential harms. If a neonate becomes unwell and requires intensive care, they should be nursed with droplet precautions in a closed incubator in a negative pressure room. The management is dictated by the presenting signs and symptoms. It would appear the presenting symptoms in the neonate are mild, with low-grade fever and gastrointestinal signs such as poor feeding and vomiting. The respiratory symptoms are also limited to mild tachypnoea and/or tachycardia. However, as there has been a presentation of seizure activity with fever a neurological examina","date":"06/09/2020","objectID":"/building-a-faster-accurate-covid-search-engine-with-transformers/:0:5","tags":["Deep Learning","Transformers","Search Engine"],"title":"Building A Faster \u0026 Accurate COVID Search Engine with Transformersü§ó","uri":"/building-a-faster-accurate-covid-search-engine-with-transformers/"},{"categories":["Text Summarization"],"content":"Abstract\r\rIn this article, you will learn how to train a T5 model for text generation - to generate title given a research paper‚Äôs abstract or summary using Transformersü§ó\r\r ","date":"01/09/2020","objectID":"/transformers-generating-arxiv-papers-title-from-abstracts/:0:0","tags":["Deep Learning","Transformers","T5 Model","Summarization"],"title":"Training a T5 Transformer Model - Generating Titles from ArXiv Paper's Abstracts using ü§óTransformers","uri":"/transformers-generating-arxiv-papers-title-from-abstracts/"},{"categories":["Text Summarization"],"content":"Introduction T5 model is a Sequence-to-Sequence model. A Sequence-to-Sequence model is fully capable to perform any text to text conversion task. What does it mean? - It means that a T5 model can take any input text and convert it into any output text. Such text-to-text conversion is useful in NLP tasks like language translation, summarization, text generation etc. For this tutorial, We will take research paper‚Äôs abstract or brief summary as our input text and its corrosponding paper‚Äôs title as output text and feed it to a T5 model to train. Once the model is trained, it will be able to generate the paper‚Äôs title based on the abstract. So, let‚Äôs dive in. ","date":"01/09/2020","objectID":"/transformers-generating-arxiv-papers-title-from-abstracts/:0:1","tags":["Deep Learning","Transformers","T5 Model","Summarization"],"title":"Training a T5 Transformer Model - Generating Titles from ArXiv Paper's Abstracts using ü§óTransformers","uri":"/transformers-generating-arxiv-papers-title-from-abstracts/"},{"categories":["Text Summarization"],"content":"Dataset ArXiv has recently open-sourced a monstrous dataset of 1.7M research papers on Kaggle. Go to Dataset. We will use its abstract and title columns to train our model. title: This column represents the title of the research paper abstract: This column represents brief summary of the research paper. This will be a supervised training where abstract is our independent variable X while title is our dependent variable y. ","date":"01/09/2020","objectID":"/transformers-generating-arxiv-papers-title-from-abstracts/:0:2","tags":["Deep Learning","Transformers","T5 Model","Summarization"],"title":"Training a T5 Transformer Model - Generating Titles from ArXiv Paper's Abstracts using ü§óTransformers","uri":"/transformers-generating-arxiv-papers-title-from-abstracts/"},{"categories":["Text Summarization"],"content":"Code Note\r\rWe will use Kaggle notebook to write our code so that we can leverage free GPU.\r\r First, lets install all the dependencies - We will work with latest stable pytorch 1.6. ! pip uninstall torch torchvision -y ! pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html !pip install -U transformers !pip install -U simpletransformers let‚Äôs load the data The format of the data is a nested json import json data_file = '../input/arxiv/arxiv-metadata-oai-snapshot.json' # Helper function to load the dataset def get_metadata(): with open(data_file, 'r') as f: for line in f: yield line # let's see the first row of the data metadata = get_metadata() for paper in metadata: paper_dict = json.loads(paper) print('Title: {}\\n\\nAbstract: {}\\nRef: {}'.format(paper_dict.get('title'), paper_dict.get('abstract'), paper_dict.get('journal-ref'))) # print(paper) break Title: Calculation of prompt diphoton production cross sections at Tevatron and LHC energies Abstract: A fully differential calculation in perturbative quantum chromodynamics is presented for the production of massive photon pairs at hadron colliders. All next-to-leading order perturbative contributions from quark-antiquark, gluon-(anti)quark, and gluon-gluon subprocesses are included, as well as all-orders resummation of initial-state gluon radiation valid at next-to-next-to-leading logarithmic accuracy. The region of phase space is specified in which the calculation is most reliable. Good agreement is demonstrated with data from the Fermilab Tevatron, and predictions are made for more detailed tests with CDF and DO data. Predictions are shown for distributions of diphoton pairs produced at the energy of the Large Hadron Collider (LHC). Distributions of the diphoton pairs from the decay of a Higgs boson are contrasted with those produced from QCD processes at the LHC, showing that enhanced sensitivity to the signal can be obtained with judicious selection of events. Ref: Phys.Rev.D76:013009,2007 We have taken 3 attributes of the dataset: Title, Abstract and Ref. Ref is important because the 4 characters of its value give us the year in which the paper was published. We will take last 5 years ArXiv papers (2016-2020) due to Kaggle‚Äôc compute limits titles = [] abstracts = [] years = [] metadata = get_metadata() for paper in metadata: paper_dict = json.loads(paper) ref = paper_dict.get('journal-ref') try: year = int(ref[-4:]) if 2016 \u003c year \u003c 2021: years.append(year) titles.append(paper_dict.get('title')) abstracts.append(paper_dict.get('abstract')) except: pass len(titles), len(abstracts), len(years) (25625, 25625, 25625) So, we have around 25K research papers published from 2016 to 2020. Next, we will convert this data into pandas dataframe and then we will use this data to train our T5 model papers = pd.DataFrame({ 'title': titles, 'abstract': abstracts, 'year': years }) papers.head() ","date":"01/09/2020","objectID":"/transformers-generating-arxiv-papers-title-from-abstracts/:0:3","tags":["Deep Learning","Transformers","T5 Model","Summarization"],"title":"Training a T5 Transformer Model - Generating Titles from ArXiv Paper's Abstracts using ü§óTransformers","uri":"/transformers-generating-arxiv-papers-title-from-abstracts/"},{"categories":["Text Summarization"],"content":"Training We will use simpletransformers library to train T5 model. This library is based on the Transformers library by HuggingFace. SimpleTransformers lets you quickly train and evaluate Transformer models. Only 3 lines of code are needed to initialize a model, train the model, and evaluate a model. You can read more about it here: https://github.com/ThilinaRajapakse/simpletransformers Input Data Simpletransformers implementation of T5 model expects a data to be a dataframe with 3 columns: \u003cprefix\u003e, \u003cinput_text\u003e, \u003ctarget_text\u003e \u003cprefix\u003e: A string indicating the task to perform. (E.g. ‚Äúquestion‚Äù, ‚Äústsb‚Äù, ‚Äúsummarization‚Äù) \u003cinput_text\u003e: The input text sequence (we will use Paper‚Äôs abstract as input_text ) \u003ctarget_text\u003e: The target sequence (we will use Paper‚Äôs title as output_text ) You can read about the data format: https://github.com/ThilinaRajapakse/simpletransformers#t5-transformer # Adding \u003cinput_text\u003e and \u003ctarget_text\u003e columns papers = papers[['title','abstract']] papers.columns = ['target_text', 'input_text'] # Adding \u003cprefix\u003e columns papers['prefix'] = \"summarize\" # splitting the data into training and test dataset eval_df = papers.sample(frac=0.2, random_state=101) train_df = papers.drop(eval_df.index) train_df.shape, eval_df.shape ((20500, 2), (5125, 2)) We have around 20K research papers for training and 5K papers for evaluation. Setting Training Parameters and Start Training We will train our T5 model with very bare minimum num_train_epochs=4, train_batch_size=16 to fit into Kaggle‚Äôs compute limits. Feel free to play around these training parameters. import logging import pandas as pd from simpletransformers.t5 import T5Model logging.basicConfig(level=logging.INFO) transformers_logger = logging.getLogger(\"transformers\") transformers_logger.setLevel(logging.WARNING) # T5 Training parameters model_args = { \"reprocess_input_data\": True, \"overwrite_output_dir\": True, \"max_seq_length\": 512, \"train_batch_size\": 16, \"num_train_epochs\": 4, } # Create T5 Model model = T5Model(\"t5-small\", args=model_args, use_cuda=True) # Train T5 Model on new task model.train_model(train_df) # Evaluate T5 Model on new task results = model.eval_model(eval_df) print(results) {'eval_loss': 2.103029722170599} It took around 4 hours to train for 4 epochs and with batch_size of 16. And we get a loss of 2.103 on our test data. ","date":"01/09/2020","objectID":"/transformers-generating-arxiv-papers-title-from-abstracts/:0:4","tags":["Deep Learning","Transformers","T5 Model","Summarization"],"title":"Training a T5 Transformer Model - Generating Titles from ArXiv Paper's Abstracts using ü§óTransformers","uri":"/transformers-generating-arxiv-papers-title-from-abstracts/"},{"categories":["Text Summarization"],"content":"We‚Äôre Done ! Let‚Äôs see how our model performs in generating paper‚Äôs titles Example 1 random_num = 350 actual_title = eval_df.iloc[random_num]['target_text'] actual_abstract = [\"summarize: \"+eval_df.iloc[random_num]['input_text']] predicted_title = model.predict(actual_abstract) print(f'Actual Title: {actual_title}') print(f'Predicted Title: {predicted_title}') print(f'Actual Abstract: {actual_abstract}') Actual Title: Cooperative Passive Coherent Location: A Promising 5G Service to Support Road Safety Predicted Title: ['CPCL: a distributed MIMO radar service for public users'] Actual Abstract: ['summarize: 5G promises many new vertical service areas beyond simple communication and\\ndata transfer. We propose CPCL (cooperative passive coherent location), a\\ndistributed MIMO radar service, which can be offered by mobile radio network\\noperators as a service for public user groups. CPCL comes as an inherent part\\nof the radio network and takes advantage of the most important key features\\nproposed for 5G. It extends the well-known idea of passive radar (also known as\\npassive coherent location, PCL) by introducing cooperative principles. These\\nrange from cooperative, synchronous radio signaling, and MAC up to radar data\\nfusion on sensor and scenario levels. By using software-defined radio and\\nnetwork paradigms, as well as real-time mobile edge computing facilities\\nintended for 5G, CPCL promises to become a ubiquitous radar service which may\\nbe adaptive, reconfigurable, and perhaps cognitive. As CPCL makes double use of\\nradio resources (both in terms of frequency bands and hardware), it can be\\nconsidered a green technology. Although we introduce the CPCL idea from the\\nviewpoint of vehicle-to-vehicle/infrastructure (V2X) communication, it can\\ndefinitely also be applied to many other applications in industry, transport,\\nlogistics, and for safety and security applications.\\n'] Example 2 random_num = 478 actual_title = eval_df.iloc[random_num]['target_text'] actual_abstract = [\"summarize: \"+eval_df.iloc[random_num]['input_text']] predicted_title = model.predict(actual_abstract) print(f'Actual Title: {actual_title}') print(f'Predicted Title: {predicted_title}') print(f'Actual Abstract: {actual_abstract}') Actual Title: Test Model Coverage Analysis under Uncertainty Predicted Title: ['Probabilistic aggregate coverage analysis for model-based testing'] Actual Abstract: ['summarize: In model-based testing (MBT) we may have to deal with a non-deterministic\\nmodel, e.g. because abstraction was applied, or because the software under test\\nitself is non-deterministic. The same test case may then trigger multiple\\npossible execution paths, depending on some internal decisions made by the\\nsoftware. Consequently, performing precise test analyses, e.g. to calculate the\\ntest coverage, are not possible. This can be mitigated if developers can\\nannotate the model with estimated probabilities for taking each transition. A\\nprobabilistic model checking algorithm can subsequently be used to do simple\\nprobabilistic coverage analysis. However, in practice developers often want to\\nknow what the achieved aggregate coverage, which unfortunately cannot be\\nre-expressed as a standard model checking problem. This paper presents an\\nextension to allow efficient calculation of probabilistic aggregate coverage,\\nand moreover also in combination with k-wise coverage.\\n'] Example 3 random_num = 999 actual_title = eval_df.iloc[random_num]['target_text'] actual_abstract = [\"summarize: \"+eval_df.iloc[random_num]['input_text']] predicted_title = model.predict(actual_abstract) print(f'Actual Title: {actual_title}') print(f'Predicted Title: {predicted_title}') print(f'Actual Abstract: {actual_abstract}') Actual Title: Computational intelligence for qualitative coaching diagnostics: Automated assessment of tennis swings to improve performance and safety Predicted Title: ['Personalized qualitative feedback for tennis swing technique using 3D video'] Actual Abs","date":"01/09/2020","objectID":"/transformers-generating-arxiv-papers-title-from-abstracts/:0:5","tags":["Deep Learning","Transformers","T5 Model","Summarization"],"title":"Training a T5 Transformer Model - Generating Titles from ArXiv Paper's Abstracts using ü§óTransformers","uri":"/transformers-generating-arxiv-papers-title-from-abstracts/"},{"categories":["Question Answering"],"content":"Abstract\r\rIn this article, you will learn how to fetch contextual answers in a huge corpus of documents using Transformersü§ó\r\r ","date":"19/08/2020","objectID":"/transformers-building-question-answers-model-at-scale/:0:0","tags":["Deep Learning","Transformers","Question \u0026 Answering"],"title":"Building Question Answering Model at Scale using ü§óTransformers","uri":"/transformers-building-question-answers-model-at-scale/"},{"categories":["Question Answering"],"content":"Introduction We will build a neural question and answering system using transformers models (RoBERTa). This approach is capable to perform Q\u0026A across millions of documents in few seconds. ","date":"19/08/2020","objectID":"/transformers-building-question-answers-model-at-scale/:0:1","tags":["Deep Learning","Transformers","Question \u0026 Answering"],"title":"Building Question Answering Model at Scale using ü§óTransformers","uri":"/transformers-building-question-answers-model-at-scale/"},{"categories":["Question Answering"],"content":"Data For this tutorial, I will use ArXiV‚Äôs research papers abstracts to do Q\u0026A. The data is on Kaggle. Go to dataset. The dataset has many columns like id author title categories but the columns we will be interested in are title and abstract. abstract contains a long summary of the research paper. We will use this column to build our Question \u0026 Answer model. Let‚Äôs dive into the code. ","date":"19/08/2020","objectID":"/transformers-building-question-answers-model-at-scale/:0:2","tags":["Deep Learning","Transformers","Question \u0026 Answering"],"title":"Building Question Answering Model at Scale using ü§óTransformers","uri":"/transformers-building-question-answers-model-at-scale/"},{"categories":["Question Answering"],"content":"Code Note\r\rWe will use Kaggle notebook to write our code so that we can leverage free GPU.\r\r The format of the data is a nested json. We will limit our analysis to just 50,000 documents because of the compute limit on Kaggle to avoid out of memory error. import json data = [] with open(\"/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json\", 'r') as f: for line in f: data.append(json.loads(line)) # Limiting our analysis to 50K documents to avoid memory error data = pd.DataFrame(data[:50000]) # Let's look at the data data.head() We will use abstract column to train our QA model. ","date":"19/08/2020","objectID":"/transformers-building-question-answers-model-at-scale/:0:3","tags":["Deep Learning","Transformers","Question \u0026 Answering"],"title":"Building Question Answering Model at Scale using ü§óTransformers","uri":"/transformers-building-question-answers-model-at-scale/"},{"categories":["Question Answering"],"content":"Haystack Now, Welcome Haystack! The secret sauce behind scaling up to thousands of documents is Haystack. Haystack helps you scale QA models to large collections of documents! You can read more about this amazing library here https://github.com/deepset-ai/haystack For installation: ! pip install git+https://github.com/deepset-ai/haystack.git But just to give a background, there are 3 major components to Haystack. Document Store: Database storing the documents for our search. We recommend Elasticsearch, but have also more light-weight options for fast prototyping (SQL or In-Memory). Retriever: Fast, simple algorithm that identifies candidate passages from a large collection of documents. Algorithms include TF-IDF or BM25, custom Elasticsearch queries, and embedding-based approaches. The Retriever helps to narrow down the scope for Reader to smaller units of text where a given question could be answered. Reader: Powerful neural model that reads through texts in detail to find an answer. Use diverse models like BERT, RoBERTa or XLNet trained via FARM or Transformers on SQuAD like tasks. The Reader takes multiple passages of text as input and returns top-n answers with corresponding confidence scores. You can just load a pretrained model from Hugging Face‚Äôs model hub or fine-tune it to your own domain data. And then there is Finder which glues together a Reader and a Retriever as a pipeline to provide an easy-to-use question answering interface. Now, we can setup Haystack in 3 steps: Install haystack and import its required modules Setup DocumentStore Setup Retriever, Reader and Finder 1. Install haystack Let‚Äôs install haystack and import all the required modules # installing haystack ! pip install git+https://github.com/deepset-ai/haystack.git # importing necessary dependencies from haystack import Finder from haystack.indexing.cleaning import clean_wiki_text from haystack.indexing.utils import convert_files_to_dicts, fetch_archive_from_http from haystack.reader.farm import FARMReader from haystack.reader.transformers import TransformersReader from haystack.utils import print_answers 2. Setting up DocumentStore Haystack finds answers to queries within the documents stored in a DocumentStore. The current implementations of DocumentStore include ElasticsearchDocumentStore, SQLDocumentStore, and InMemoryDocumentStore. But they recommend ElasticsearchDocumentStore because as it comes preloaded with features like full-text queries, BM25 retrieval, and vector storage for text embeddings. So - Let‚Äôs set up a ElasticsearchDocumentStore. ! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q ! tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz ! chown -R daemon:daemon elasticsearch-7.6.2 import os from subprocess import Popen, PIPE, STDOUT es_server = Popen(['elasticsearch-7.6.2/bin/elasticsearch'], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1) # as daemon ) # wait until ES has started ! sleep 30 # initiating ElasticSearch from haystack.database.elasticsearch import ElasticsearchDocumentStore document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\") Once ElasticsearchDocumentStore is setup, we will write our documents/texts to the DocumentStore. Writing documents to ElasticsearchDocumentStore requires a format - List of dictionaries as shown below: [ {\"name\": \"\u003csome-document-name\u003e, \"text\": \"\u003cthe-actual-text\u003e\"}, {\"name\": \"\u003csome-document-name\u003e, \"text\": \"\u003cthe-actual-text\u003e\"} {\"name\": \"\u003csome-document-name\u003e, \"text\": \"\u003cthe-actual-text\u003e\"} ] (Optionally: you can also add more key-value-pairs here, that will be indexed as fields in Elasticsearch and can be accessed later for filtering or shown in the responses of the Finder) We will use title column to pass as name and abstract column to pass as the text # Now, let's write the dicts containing documents to our DB. document_store.write_documents(data[['title', 'abstract']].rename(columns={'tit","date":"19/08/2020","objectID":"/transformers-building-question-answers-model-at-scale/:0:4","tags":["Deep Learning","Transformers","Question \u0026 Answering"],"title":"Building Question Answering Model at Scale using ü§óTransformers","uri":"/transformers-building-question-answers-model-at-scale/"},{"categories":["Question Answering"],"content":"We‚Äôre done ! Once we have our Finder ready, we are all set to see our model fetching answers for us based on the question. Below is the list of questions that I was asking the model prediction = finder.get_answers(question=\"What do we know about symbiotic stars\", top_k_retriever=10, top_k_reader=2) result = print_answers(prediction, details=\"minimal\") Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.17 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.83 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.71 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.75 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.78 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.83 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01\u003c00:00, 1.08s/ Batches] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01\u003c00:00, 1.09s/ Batches] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.83 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.57 Batches/s] [ { 'answer': 'Their observed population in the\\n' 'Galaxy is however poorly known, and is one to three orders ' 'of magnitudes\\n' 'smaller than the predicted population size', 'context': ' The study of symbiotic stars is essential to understand ' 'important aspects of\\n' 'stellar evolution in interacting binaries. Their observed ' 'population in the\\n' 'Galaxy is however poorly known, and is one to three orders ' 'of magnitudes\\n' 'smaller than the predicted population size. IPHAS, the INT ' 'Photometric Halpha\\n' 'survey of the Northern Galactic plane, gives us the ' 'opportunity to make a\\n' 'systematic, complete search for symbiotic stars in a ' 'magnitude-limited volume,\\n' 'and discover a significant number of new '}, { 'answer': 'Their observed population in the\\n' 'Galaxy is however poorly known, and is one to three orders ' 'of magnitudes\\n' 'smaller than the predicted population size', 'context': ' The study of symbiotic stars is essential to understand ' 'important aspects of\\n' 'stellar evolution in interacting binaries. Their observed ' 'population in the\\n' 'Galaxy is however poorly known, and is one to three orders ' 'of magnitudes\\n' 'smaller than the predicted population size. IPHAS, the INT ' 'Photometric Halpha\\n' 'survey of the Northern Galactic plane, gives us the ' 'opportunity to make a\\n' 'systematic, complete search for symbiotic stars in a ' 'magnitude-limited volume,\\n' 'and discover a significant number of new '}] Let‚Äôs try few more examples - prediction = finder.get_answers(question=\"How is structure of event horizon linked with Morse theory?\", top_k_retriever=10, top_k_reader=2) result = print_answers(prediction, details=\"minimal\") Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.17 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.83 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.71 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.75 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.78 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.83 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01\u003c00:00, 1.08s/ Batches] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01\u003c00:00, 1.09s/ Batches] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.83 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.57 Batches/s] [ { 'answer': 'in terms\\nof the Morse theory', 'context': ' The topological structure of the event horizon has been ' 'investigated in terms\\n' 'of the Morse theory. The elementary process of topological ' 'evolution can be\\n' 'understood as a handle attachment. It has been found that ' 'there are certain\\n' 'constraints on the nature of black hole topological ' 'evolution: (i) There are n\\n' 'kinds of handle attachments in (n+1)-dimensional black ' 'hole space-times. (ii)\\n' 'Handles are further classified as ","date":"19/08/2020","objectID":"/transformers-building-question-answers-model-at-scale/:0:5","tags":["Deep Learning","Transformers","Question \u0026 Answering"],"title":"Building Question Answering Model at Scale using ü§óTransformers","uri":"/transformers-building-question-answers-model-at-scale/"}]