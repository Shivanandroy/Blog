[{"categories":null,"content":"In this article, you will learn how to fine tune a T5 model with PyTorch and transformers","date":"08/02/2021","objectID":"/fine-tune-t5-transformer-with-pytorch/","tags":["Deep Learning","Transformers","T5"],"title":"Fine Tuning T5 Transformer Model with PyTorch","uri":"/fine-tune-t5-transformer-with-pytorch/"},{"categories":null,"content":"Abstract\r\rIn this article, you will learn how to fine tune a T5 transformer model using PyTorch \u0026 Transformersü§ó \r\r ","date":"08/02/2021","objectID":"/fine-tune-t5-transformer-with-pytorch/:0:0","tags":["Deep Learning","Transformers","T5"],"title":"Fine Tuning T5 Transformer Model with PyTorch","uri":"/fine-tune-t5-transformer-with-pytorch/"},{"categories":null,"content":"Introduction A T5 is an encoder-decoder model. It converts all NLP problems like language translation, summarization, text generation, question-answering, to a text-to-text task. For e.g., in case of translation, T5 accepts source text: English, as input and tries to convert it into target text: Serbian: source text target text Hey, there! –•–µ—ò —Ç–∞–º–æ! I‚Äôm going to train a T5 model with PyTorch –û–±—É—á–∏—õ—É –º–æ–¥–µ–ª –¢5 —Å–∞ –ü–∏–¢–æ—Ä—Ü—Ö-–æ–º In case of summarization, source text or input can be a long description and target text can just be a one line summary. source text target text ‚ÄúSaurav Kant, an alumnus of upGrad and IIIT-B‚Äôs PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad‚Äôs 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad‚Äôs Online Power Learning has powered 3 lakh+ careers.‚Äù upGrad learner switches to career in ML \u0026 Al with 90% salary hike In this article, we will take a pretrained T5-base model and fine tune it to generate a one line summary of news articles using PyTorch. ","date":"08/02/2021","objectID":"/fine-tune-t5-transformer-with-pytorch/:1:0","tags":["Deep Learning","Transformers","T5"],"title":"Fine Tuning T5 Transformer Model with PyTorch","uri":"/fine-tune-t5-transformer-with-pytorch/"},{"categories":null,"content":"Data We will take a news summary dataset: It has 2 columns: text : article content headlines : one line summary of article content import pandas as pd path = \"https://raw.githubusercontent.com/Shivanandroy/T5-Finetuning-PyTorch/main/data/news_summary.csv\" df = pd.read_csv(path) dh.head() text headlines ‚ÄúKunal Shah‚Äôs credit card bill payment platform, CRED, gave users a chance to win free food from Swiggy for one year. Pranav Kaushik, a Delhi techie, bagged this reward after spending 2000 CRED coins. Users get one CRED coin per rupee of bill paid, which can be used to avail rewards from brands like Ixigo, BookMyShow, UberEats, Cult.Fit and more.‚Äù Delhi techie wins free food from Swiggy for one year on CRED New Zealand defeated India by 8 wickets in the fourth ODI at Hamilton on Thursday to win their first match of the five-match ODI series. India lost an international match under Rohit Sharma‚Äôs captaincy after 12 consecutive victories dating back to March 2018. The match witnessed India getting all out for 92, their seventh lowest total in ODI cricket history.\" New Zealand end Rohit Sharma-led India‚Äôs 12-match winning streak With Aegon Life iTerm Insurance plan, customers can enjoy tax benefits on your premiums paid and save up to √¢\\x82¬π46,800^ on taxes. The plan provides life cover up to the age of 100 years. Also, customers have options to insure against Critical Illnesses, Disability and Accidental Death Benefit Rider with a life cover up to the age of 80 years.' Aegon life iTerm insurance plan helps customers save tax Isha Ghosh, an 81-year-old member of Bharat Scouts and Guides (BSG), has been imparting physical and mental training to schoolchildren in Jharkhand for several decades. Chaibasa-based Ghosh reportedly walks seven kilometres daily and spends eight hours conducting physical training, apart from climbing and yoga sessions. She says, ‚ÄúOne should do something for society till one's last breath.\"' 81-yr-old woman conducts physical training in J‚Äôkhand schools ","date":"08/02/2021","objectID":"/fine-tune-t5-transformer-with-pytorch/:2:0","tags":["Deep Learning","Transformers","T5"],"title":"Fine Tuning T5 Transformer Model with PyTorch","uri":"/fine-tune-t5-transformer-with-pytorch/"},{"categories":null,"content":"Let‚Äôs Code PyTorch has a standard way to train any deep learning model. We will first start by writing a Dataset class, followed by training, validation steps and then a main T5Trainer function that will fine-tune our model. But first let‚Äôs install all the dependent modules and import them ","date":"08/02/2021","objectID":"/fine-tune-t5-transformer-with-pytorch/:3:0","tags":["Deep Learning","Transformers","T5"],"title":"Fine Tuning T5 Transformer Model with PyTorch","uri":"/fine-tune-t5-transformer-with-pytorch/"},{"categories":null,"content":"Import Libraries !pip install sentencepiece !pip install transformers !pip install torch !pip install rich[jupyter] # Importing libraries import os import numpy as np import pandas as pd import torch import torch.nn.functional as F from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler import os # Importing the T5 modules from huggingface/transformers from transformers import T5Tokenizer, T5ForConditionalGeneration # rich: for a better display on terminal from rich.table import Column, Table from rich import box from rich.console import Console # define a rich console logger console = Console(record=True) # to display dataframe in ASCII format def display_df(df): \"\"\"display dataframe in ASCII format\"\"\" console = Console() table = Table( Column(\"source_text\", justify=\"center\"), Column(\"target_text\", justify=\"center\"), title=\"Sample Data\", pad_edge=False, box=box.ASCII, ) for i, row in enumerate(df.values.tolist()): table.add_row(row[0], row[1]) console.print(table) # training logger to log training progress training_logger = Table( Column(\"Epoch\", justify=\"center\"), Column(\"Steps\", justify=\"center\"), Column(\"Loss\", justify=\"center\"), title=\"Training Status\", pad_edge=False, box=box.ASCII, ) # Setting up the device for GPU usage from torch import cuda device = 'cuda' if cuda.is_available() else 'cpu' ","date":"08/02/2021","objectID":"/fine-tune-t5-transformer-with-pytorch/:3:1","tags":["Deep Learning","Transformers","T5"],"title":"Fine Tuning T5 Transformer Model with PyTorch","uri":"/fine-tune-t5-transformer-with-pytorch/"},{"categories":null,"content":"Dataset Class We will write a Dataset class for reading our dataset and loading it into the dataloader and then feed it to the neural network for fine tuning the model. This class will take 6 arguments as input: dataframe (pandas.DataFrame): Input dataframe tokenizer (transformers.tokenizer): T5 tokenizer source_len (int): Max length of source text target_len (int): Max length of target text source_text (str): column name of source text target_text (str) : column name of target text This class will have 2 methods: __len__: returns the length of the dataframe __getitem__: return the input ids, attention masks and target ids class YourDataSetClass(Dataset): \"\"\" Creating a custom dataset for reading the dataset and loading it into the dataloader to pass it to the neural network for finetuning the model \"\"\" def __init__( self, dataframe, tokenizer, source_len, target_len, source_text, target_text ): \"\"\" Initializes a Dataset class Args: dataframe (pandas.DataFrame): Input dataframe tokenizer (transformers.tokenizer): Transformers tokenizer source_len (int): Max length of source text target_len (int): Max length of target text source_text (str): column name of source text target_text (str): column name of target text \"\"\" self.tokenizer = tokenizer self.data = dataframe self.source_len = source_len self.summ_len = target_len self.target_text = self.data[target_text] self.source_text = self.data[source_text] def __len__(self): \"\"\"returns the length of dataframe\"\"\" return len(self.target_text) def __getitem__(self, index): \"\"\"return the input ids, attention masks and target ids\"\"\" source_text = str(self.source_text[index]) target_text = str(self.target_text[index]) # cleaning data so as to ensure data is in string type source_text = \" \".join(source_text.split()) target_text = \" \".join(target_text.split()) source = self.tokenizer.batch_encode_plus( [source_text], max_length=self.source_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors=\"pt\", ) target = self.tokenizer.batch_encode_plus( [target_text], max_length=self.summ_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors=\"pt\", ) source_ids = source[\"input_ids\"].squeeze() source_mask = source[\"attention_mask\"].squeeze() target_ids = target[\"input_ids\"].squeeze() target_mask = target[\"attention_mask\"].squeeze() return { \"source_ids\": source_ids.to(dtype=torch.long), \"source_mask\": source_mask.to(dtype=torch.long), \"target_ids\": target_ids.to(dtype=torch.long), \"target_ids_y\": target_ids.to(dtype=torch.long), } ","date":"08/02/2021","objectID":"/fine-tune-t5-transformer-with-pytorch/:3:2","tags":["Deep Learning","Transformers","T5"],"title":"Fine Tuning T5 Transformer Model with PyTorch","uri":"/fine-tune-t5-transformer-with-pytorch/"},{"categories":null,"content":"Train steps train function will the put model on training mode, generate outputs and calculate loss This will take 6 arguments as input: epoch: epoch tokenizer: T5 tokenizer model: T5 model loader: Train Dataloader optimizer: Optimizer def train(epoch, tokenizer, model, device, loader, optimizer): \"\"\" Function to be called for training with the parameters passed from main function \"\"\" model.train() for _, data in enumerate(loader, 0): y = data[\"target_ids\"].to(device, dtype=torch.long) y_ids = y[:, :-1].contiguous() lm_labels = y[:, 1:].clone().detach() lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100 ids = data[\"source_ids\"].to(device, dtype=torch.long) mask = data[\"source_mask\"].to(device, dtype=torch.long) outputs = model( input_ids=ids, attention_mask=mask, decoder_input_ids=y_ids, labels=lm_labels, ) loss = outputs[0] if _ % 10 == 0: training_logger.add_row(str(epoch), str(_), str(loss)) console.print(training_logger) optimizer.zero_grad() loss.backward() optimizer.step() ","date":"08/02/2021","objectID":"/fine-tune-t5-transformer-with-pytorch/:3:3","tags":["Deep Learning","Transformers","T5"],"title":"Fine Tuning T5 Transformer Model with PyTorch","uri":"/fine-tune-t5-transformer-with-pytorch/"},{"categories":null,"content":"Validation steps validate function is same as the train function, but for the validation data def validate(epoch, tokenizer, model, device, loader): \"\"\" Function to evaluate model for predictions \"\"\" model.eval() predictions = [] actuals = [] with torch.no_grad(): for _, data in enumerate(loader, 0): y = data['target_ids'].to(device, dtype = torch.long) ids = data['source_ids'].to(device, dtype = torch.long) mask = data['source_mask'].to(device, dtype = torch.long) generated_ids = model.generate( input_ids = ids, attention_mask = mask, max_length=150, num_beams=2, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True ) preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids] target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y] if _%10==0: console.print(f'Completed {_}') predictions.extend(preds) actuals.extend(target) return predictions, actuals ","date":"08/02/2021","objectID":"/fine-tune-t5-transformer-with-pytorch/:3:4","tags":["Deep Learning","Transformers","T5"],"title":"Fine Tuning T5 Transformer Model with PyTorch","uri":"/fine-tune-t5-transformer-with-pytorch/"},{"categories":null,"content":"T5 Trainer T5Trainer is our main function. It accepts input data, model type, model paramters to fine-tune the model. Under the hood, it utilizes, our Dataset class for data handling, train function to fine tune the model, validate to evaluate the model. T5Trainer will have 5 arguments: dataframe: Input dataframe source_text: Column name of the input text i.e. article content target_text: Column name of the taregt text i.e. one line summary model_params: T5 model parameters output_dir: Output directory to save fine tuned T5 model. def T5Trainer( dataframe, source_text, target_text, model_params, output_dir=\"./outputs/\" ): \"\"\" T5 trainer \"\"\" # Set random seeds and deterministic pytorch for reproducibility torch.manual_seed(model_params[\"SEED\"]) # pytorch random seed np.random.seed(model_params[\"SEED\"]) # numpy random seed torch.backends.cudnn.deterministic = True # logging console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\") # tokenzier for encoding the text tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"]) # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. # Further this model is sent to device (GPU/TPU) for using the hardware. model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"]) model = model.to(device) # logging console.log(f\"[Data]: Reading data...\\n\") # Importing the raw dataset dataframe = dataframe[[source_text, target_text]] display_df(dataframe.head(2)) # Creation of Dataset and Dataloader # Defining the train size. So 80% of the data will be used for training and the rest for validation. train_size = 0.8 train_dataset = dataframe.sample(frac=train_size, random_state=model_params[\"SEED\"]) val_dataset = dataframe.drop(train_dataset.index).reset_index(drop=True) train_dataset = train_dataset.reset_index(drop=True) console.print(f\"FULL Dataset: {dataframe.shape}\") console.print(f\"TRAIN Dataset: {train_dataset.shape}\") console.print(f\"TEST Dataset: {val_dataset.shape}\\n\") # Creating the Training and Validation dataset for further creation of Dataloader training_set = YourDataSetClass( train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text, ) val_set = YourDataSetClass( val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text, ) # Defining the parameters for creation of dataloaders train_params = { \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"], \"shuffle\": True, \"num_workers\": 0, } val_params = { \"batch_size\": model_params[\"VALID_BATCH_SIZE\"], \"shuffle\": False, \"num_workers\": 0, } # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model. training_loader = DataLoader(training_set, **train_params) val_loader = DataLoader(val_set, **val_params) # Defining the optimizer that will be used to tune the weights of the network in the training session. optimizer = torch.optim.Adam( params=model.parameters(), lr=model_params[\"LEARNING_RATE\"] ) # Training loop console.log(f\"[Initiating Fine Tuning]...\\n\") for epoch in range(model_params[\"TRAIN_EPOCHS\"]): train(epoch, tokenizer, model, device, training_loader, optimizer) console.log(f\"[Saving Model]...\\n\") # Saving the model after training path = os.path.join(output_dir, \"model_files\") model.save_pretrained(path) tokenizer.save_pretrained(path) # evaluating test dataset console.log(f\"[Initiating Validation]...\\n\") for epoch in range(model_params[\"VAL_EPOCHS\"]): predictions, actuals = validate(epoch, tokenizer, model, device, val_loader) final_df = pd.DataFrame({\"Generated Text\": predictions, \"Actual Text\": actuals}) final_df.to_csv(os.path.join(output_dir, \"predictions.csv\")) console.save_text(os.path.join(output_dir, \"logs.txt\")) console.log(f\"[Validation Completed.]\\n\") console.print( f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\" ) cons","date":"08/02/2021","objectID":"/fine-tune-t5-transformer-with-pytorch/:3:5","tags":["Deep Learning","Transformers","T5"],"title":"Fine Tuning T5 Transformer Model with PyTorch","uri":"/fine-tune-t5-transformer-with-pytorch/"},{"categories":null,"content":"Model Parameters model_params is a dictionary containing model paramters for T5 training: MODEL: \"t5-base\", model_type: t5-base/t5-large TRAIN_BATCH_SIZE: 8, training batch size VALID_BATCH_SIZE: 8, validation batch size TRAIN_EPOCHS: 3, number of training epochs VAL_EPOCHS: 1, number of validation epochs LEARNING_RATE: 1e-4, learning rate MAX_SOURCE_TEXT_LENGTH: 512, max length of source text MAX_TARGET_TEXT_LENGTH: 50, max length of target text SEED: 42, set seed for reproducibility # let's define model parameters specific to T5 model_params = { \"MODEL\": \"t5-base\", # model_type: t5-base/t5-large \"TRAIN_BATCH_SIZE\": 8, # training batch size \"VALID_BATCH_SIZE\": 8, # validation batch size \"TRAIN_EPOCHS\": 3, # number of training epochs \"VAL_EPOCHS\": 1, # number of validation epochs \"LEARNING_RATE\": 1e-4, # learning rate \"MAX_SOURCE_TEXT_LENGTH\": 512, # max length of source text \"MAX_TARGET_TEXT_LENGTH\": 50, # max length of target text \"SEED\": 42, # set seed for reproducibility } ","date":"08/02/2021","objectID":"/fine-tune-t5-transformer-with-pytorch/:3:6","tags":["Deep Learning","Transformers","T5"],"title":"Fine Tuning T5 Transformer Model with PyTorch","uri":"/fine-tune-t5-transformer-with-pytorch/"},{"categories":null,"content":"Let‚Äôs call T5Trainer # T5 accepts prefix of the task to be performed: # Since we are summarizing, let's add summarize to source text as a prefix df[\"text\"] = \"summarize: \" + df[\"text\"] T5Trainer( dataframe=df, source_text=\"text\", target_text=\"headlines\", model_params=model_params, output_dir=\"outputs\", ) Training Status +--------------------------------------------------------------------------+ |Epoch | Steps | Loss | |------+-------+-----------------------------------------------------------| | 0 | 0 | tensor(8.5338, device='cuda:0', grad_fn=\u003cNllLossBackward\u003e)| | 0 | 10 | tensor(3.4278, device='cuda:0', grad_fn=\u003cNllLossBackward\u003e)| | 0 | 20 | tensor(3.0148, device='cuda:0', grad_fn=\u003cNllLossBackward\u003e)| | 0 | 30 | tensor(3.2338, device='cuda:0', grad_fn=\u003cNllLossBackward\u003e)| | 0 | 40 | tensor(2.5963, device='cuda:0', grad_fn=\u003cNllLossBackward\u003e)| | 1 | 0 | tensor(2.2411, device='cuda:0', grad_fn=\u003cNllLossBackward\u003e)| | 1 | 10 | tensor(1.9470, device='cuda:0', grad_fn=\u003cNllLossBackward\u003e)| | 1 | 20 | tensor(1.9091, device='cuda:0', grad_fn=\u003cNllLossBackward\u003e)| | 1 | 30 | tensor(2.0122, device='cuda:0', grad_fn=\u003cNllLossBackward\u003e)| | 1 | 40 | tensor(1.5261, device='cuda:0', grad_fn=\u003cNllLossBackward\u003e)| | 2 | 0 | tensor(1.6496, device='cuda:0', grad_fn=\u003cNllLossBackward\u003e)| | 2 | 10 | tensor(1.1971, device='cuda:0', grad_fn=\u003cNllLossBackward\u003e)| | 2 | 20 | tensor(1.6908, device='cuda:0', grad_fn=\u003cNllLossBackward\u003e)| | 2 | 30 | tensor(1.4069, device='cuda:0', grad_fn=\u003cNllLossBackward\u003e)| | 2 | 40 | tensor(2.1261, device='cuda:0', grad_fn=\u003cNllLossBackward\u003e)| +--------------------------------------------------------------------------+ ","date":"08/02/2021","objectID":"/fine-tune-t5-transformer-with-pytorch/:3:7","tags":["Deep Learning","Transformers","T5"],"title":"Fine Tuning T5 Transformer Model with PyTorch","uri":"/fine-tune-t5-transformer-with-pytorch/"},{"categories":null,"content":"Notebooks Attachments\r\r Go to Dataset. Go to Notebook \r\r","date":"08/02/2021","objectID":"/fine-tune-t5-transformer-with-pytorch/:4:0","tags":["Deep Learning","Transformers","T5"],"title":"Fine Tuning T5 Transformer Model with PyTorch","uri":"/fine-tune-t5-transformer-with-pytorch/"},{"categories":["Natural Language Understanding"],"content":"In this article, we will visualize Game of Thrones books with BERT in 3D space.","date":"13/11/2020","objectID":"/visualizing-game-of-thrones-with-bert/","tags":["Deep Learning","Game of Thrones","BERT"],"title":"Visualizing Game of Thrones with BERT","uri":"/visualizing-game-of-thrones-with-bert/"},{"categories":["Natural Language Understanding"],"content":"Abstract\r\rIn this article, we will visualize Game of Thrones books with BERT in 3D embedding space .\r\r ","date":"13/11/2020","objectID":"/visualizing-game-of-thrones-with-bert/:0:0","tags":["Deep Learning","Game of Thrones","BERT"],"title":"Visualizing Game of Thrones with BERT","uri":"/visualizing-game-of-thrones-with-bert/"},{"categories":["Natural Language Understanding"],"content":"Introduction This past weekend while watching Game of Thrones at dinner ‚Äî I had a thought! How does BERT understand Game of Thrones? The thought of visualizing all the texts of GOT books with mightly BERT in 3D space. How can we achieve this ‚Äî First ‚Äî we will extract the BERT embeddings for each word across all GOT books. Then ‚Äî reduce the dimension of BERT embeddings to visualize it in 3D And finally ‚Äî create a web application to visualize it on the browser So, Let‚Äôs get started ","date":"13/11/2020","objectID":"/visualizing-game-of-thrones-with-bert/:1:0","tags":["Deep Learning","Game of Thrones","BERT"],"title":"Visualizing Game of Thrones with BERT","uri":"/visualizing-game-of-thrones-with-bert/"},{"categories":["Natural Language Understanding"],"content":"1. Extracting BERT Embeddings for Game of Thrones Books Extracting BERT embeddings for your custom data can be intimidating at first ‚Äî but not anymore. Gary Lai has this awesome package bert-embedding which lets you extract token level embeddings without any hassle. The code looks as simple as: # installing bert-embedding !pip install bert-embedding # importing bert_embedding from bert_embedding import BertEmbedding # text to be encoded text = \"\"\"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. \"\"\" # generating sentences sentences = text.split('\\n') # instantiating BerEmbedding class bert_embedding = BertEmbedding() # passing sentences to bert_embedding model result = bert_embedding(sentences) Let‚Äôs use this package for our data ‚Äî We will extract 5 Game of Thrones books using requests ‚Äî import requests book1 = \"https://raw.githubusercontent.com/llSourcell/word_vectors_game_of_thrones-LIVE/master/data/got1.txt\" book2 = \"https://raw.githubusercontent.com/llSourcell/word_vectors_game_of_thrones-LIVE/master/data/got2.txt\" book3 = \"https://raw.githubusercontent.com/llSourcell/word_vectors_game_of_thrones-LIVE/master/data/got3.txt\" book4 = \"https://raw.githubusercontent.com/llSourcell/word_vectors_game_of_thrones-LIVE/master/data/got4.txt\" book5 = \"https://raw.githubusercontent.com/llSourcell/word_vectors_game_of_thrones-LIVE/master/data/got5.txt\" # fetch the content of the books b1 = requests.get(book1) b2 = requests.get(book2) b3 = requests.get(book3) b4 = requests.get(book4) b5 = requests.get(book5) # break it into list of sentences book1_content = [sent for sent in b1.text.splitlines() if sent != ''] book2_content = [sent for sent in b2.text.splitlines() if sent != ''] book3_content = [sent for sent in b3.text.splitlines() if sent != ''] book4_content = [sent for sent in b4.text.splitlines() if sent != ''] book5_content = [sent for sent in b5.text.splitlines() if sent != ''] Next ‚Äî We will clean the content of the books. And we will store the content as a list of sentences import re def sentence_to_wordlist(raw): clean = re.sub(‚Äú[^a-zA-Z]‚Äù,‚Äù ‚Äú, raw) words = clean.split() return words book1_sentences = [] for raw_sentence in book1_content: if len(raw_sentence) \u003e 0: book1_sentences.append(' '.join(sentence_to_wordlist(raw_sentence))) book2_sentences = [] for raw_sentence in book2_content: if len(raw_sentence) \u003e 0: book2_sentences.append(' '.join(sentence_to_wordlist(raw_sentence))) book3_sentences = [] for raw_sentence in book3_content: if len(raw_sentence) \u003e 0: book3_sentences.append(' '.join(sentence_to_wordlist(raw_sentence))) book4_sentences = [] for raw_sentence in book4_content: if len(raw_sentence) \u003e 0: book4_sentences.append(' '.join(sentence_to_wordlist(raw_sentence))) book5_sentences = [] for raw_sentence in book5_content: if len(raw_sentence) \u003e 0: book5_sentences.append(' '.join(sentence_to_wordlist(raw_sentence))) Once we have a clean list of sentences for each book, we can extract BERT embeddings using the code below: # imorting dependencies from bert_embedding import BertEmbedding from tqdm import tqdm_notebook import pandas as pd import mxnet as mx # bert_embedding supports GPU for faster processsing ctx = mx.gpu(0) # This function will extract BERT embeddings and store it in a # structured format i.e. dataframe def generate_bert_embeddings(sentences): bert_embedding = BertEmbedding(ctx=ctx) print(‚ÄúEncoding Sentences:‚Äù) result = bert_embedding(sentences) print(‚ÄúEncoding Finished‚Äù) df = pd.DataFrame() for i in tqdm_notebook(range(len(result))): embed = pd.DataFrame(result[i][1]) embed[‚Äòwords‚Äô] = result[i][0] df = pd.concat([df, embed]) return df book1_embedding = generate_bert_embeddings(book1_sentences) book2_embedding = generate_bert_embeddings(book2_s","date":"13/11/2020","objectID":"/visualizing-game-of-thrones-with-bert/:2:0","tags":["Deep Learning","Game of Thrones","BERT"],"title":"Visualizing Game of Thrones with BERT","uri":"/visualizing-game-of-thrones-with-bert/"},{"categories":["Natural Language Understanding"],"content":"2. Dimensionality Reduction: BERT Embeddings BERT embeddings are 768 dimension vectors i.e. we have 768 numbers to represent each word or tokens found in the books. We will reduce the dimensionality of these words from 768 to 3 ‚Äî to visualize these tokens/words in 3 dimensions using the code below: from sklearn.decomposition import TruncatedSVD from sklearn.decomposition import PCA # This function will reduce dimension of the embeddings using tSVD # and PCA def reduce_dimension(embedding_df): # Dimensionality Reduction using tSVD tsvd = TruncatedSVD(n_components=3) tsvd_3d = pd.DataFrame(tsvd.fit_transform(embedding_df.drop(‚Äòwords‚Äô, axis=1))) tsvd_3d[‚Äòwords‚Äô] = embedding_df[‚Äòwords‚Äô].values # Dimensionality reduction using PCA pca = PCA(3) pca_3d = pd.DataFrame(pca.fit_transform(embedding_df.drop(‚Äòwords‚Äô, axis=1))) pca_3d[‚Äòwords‚Äô] = embedding_df[‚Äòwords‚Äô].values return tsvd_3d, pca_3d Let‚Äôs apply the above function to our embeddings tsvd_book1, pca_book1 = reduce_dimension(book1_embedding) tsvd_book2, pca_book2 = reduce_dimension(book2_embedding) tsvd_book3, pca_book3 = reduce_dimension(book3_embedding) tsvd_book4, pca_book4 = reduce_dimension(book4_embedding) tsvd_book5, pca_book5 = reduce_dimension(book5_embedding) ü•≥ Voila! Now we have 3 dimension projection of each word in all the GOT books Extraction of BERT embeddings and dimensionality reduction can be a time-consuming process. You can download Game of Thrones BERT Embeddings from here: Download ","date":"13/11/2020","objectID":"/visualizing-game-of-thrones-with-bert/:3:0","tags":["Deep Learning","Game of Thrones","BERT"],"title":"Visualizing Game of Thrones with BERT","uri":"/visualizing-game-of-thrones-with-bert/"},{"categories":["Natural Language Understanding"],"content":"3. Building A Web App to visualize on the Browser This is the final part of this project. We will build a front end to visualize these embeddings in 3 dimensions in pure python. To do this, we will use Dash. Dash is a python framework that lets you build beautiful web-based analytical apps in pure python. No JavaScript required. You can install dash : pip install dash If you need a end-to-end article on how to build web apps in pure python with Dash, let me know in the comments A Dash application consists of 3parts ‚Äî 1. Dependencies and app instantiation This section talks about importing dependent packages and starting a Dash app # pip install dash==1.8.0 import dash import dash_html_components as html import dash_core_components as dcc import dash_table from dash.dependencies import Input, Output, State # pip install dash_bootstrap_components import dash_bootstrap_components as dbc # pip install plotly_express import plotly_express as px # pip install sd_material_ui import sd_material_ui as sd # instantiating dash application app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP,\"https://codepen.io/chriddyp/pen/brPBPO.css\"]) # adding a title to your dash app app.title=\"Visualizing Game of Thrones Using BERT\" 2. Layout It lets you define how your web application would look like - widgets, sliders, dropdowns etc. \u0026 their alignment # Loading the data: You can download it from # https://drive.google.com/open?id=1M5vHLQqCv_AB1dm9kXW4AHsA5CjcFrm1 data = pd.read_csv(\"got_embeddings.csv\") # Defining the app layout app.layout = html.Div([ html.Div(html.H4(\"Visualizing Game of Thrones with BERT\"), style={'textAlign':'center','backgroundColor':'#ff8533','color':'white','font-size':5,'padding':'1px'}), dbc.Row([ dbc.Col([ html.Br(), html.Div(\"Choose a Book\", style={'font-weight':'bold'}), sd.DropDownMenu(id='book', value='Book 1', options=[ dict(value='Book 1', primaryText='Game of Thrones 1', label='Game of Thrones 1'), dict(value='Book 2', primaryText='Game of Thrones 2'), dict(value='Book 3', primaryText='Game of Thrones 3'), dict(value='Book 4', primaryText='Game of Thrones 4'), dict(value='Book 5', primaryText='Game of Thrones 5'), ], menuStyle=dict(width=300), # controls style of the open menu listStyle=dict(height=35), selectedMenuItemStyle=dict(height=30), anchorOrigin=dict(vertical='bottom', horizontal='right')), html.Hr(), html.Div(\"Number of words:\", style={'font-weight':'bold'}), dcc.Slider(id='num_words', min=0, tooltip={'always_visible':False}, value=5000, max=10000, step=100), html.Hr(), html.Div(\"Projection\", style={'font-weight':'bold'}), dbc.RadioItems( options=[{\"label\": \"Truncated SVD\", \"value\": \"tSVD\"}, {\"label\": \"PCA\", \"value\": \"PCA\"}], value=\"tSVD\", id=\"projection\"), html.Hr(), html.Div(\"Options\", style={'font-weight':'bold'}), dbc.Checklist(options=[{\"label\": \"Show Noun Phrases\", \"value\": 'noun'}], id=\"noun_toggle\",switch=True, value=[]), dbc.Checklist(options=[{\"label\": \"Show Unique Words\", \"value\": 'unique'}], id=\"unique_toggle\",switch=True, value=['unique']), dbc.Checklist(options=[{\"label\": \"Remove Stopwords\", \"value\": 'stopword'}], id=\"stopword_toggle\",switch=True, value=['stopword']), html.Hr(), ], width=2), dbc.Col(dcc.Graph(id='visualization'), width=10) ], no_gutters=True) ]) 3. Callbacks It lets you add interactivity on your charts, visuals or buttons. @app.callback(Output(\"visualization\", \"figure\"), [ Input(\"book\", \"value\"), Input(\"num_words\", \"value\"), Input(\"projection\", \"value\"), Input(\"noun_toggle\", \"value\"), Input(\"unique_toggle\", \"value\"), Input(\"stopword_toggle\", \"value\") ], ) def on_form_change(book_num, num_words, projection, is_noun, is_unique, is_stopwords): df = data[(data.book == book_num) \u0026 (data.type == projection) \u0026 (data.length != 1)] df['word_usage'] = pd.qcut(df.frequency,5, labels=['Rare','Less Frequent','Moderate', 'Frequent','Most Frequent']) if \"noun\" in is_noun: df = df[df.pos == \"NN\"] if \"unique\" in is_unique: df = df.loc[df.words.drop_duplicates().index] if \"sto","date":"13/11/2020","objectID":"/visualizing-game-of-thrones-with-bert/:4:0","tags":["Deep Learning","Game of Thrones","BERT"],"title":"Visualizing Game of Thrones with BERT","uri":"/visualizing-game-of-thrones-with-bert/"},{"categories":["Natural Language Understanding"],"content":"ü•≥ Hooray, you‚Äôre done! Now you can explore your GOT characters in 3D. But, what did I find out from this experiment? Were all characters, food items, places, things formed seperate clusters? Which all characters were in close proximity? I will keep it for the next time ","date":"13/11/2020","objectID":"/visualizing-game-of-thrones-with-bert/:5:0","tags":["Deep Learning","Game of Thrones","BERT"],"title":"Visualizing Game of Thrones with BERT","uri":"/visualizing-game-of-thrones-with-bert/"},{"categories":["Python Packages"],"content":"NLP360 is curated list of resources related to Natural Language Processing (NLP) x updated weekly","date":"31/10/2020","objectID":"/awesome-nlp-resources/","tags":["Machine Learning","Python Packages","Awesome NLP"],"title":"NLP360 : Awesome NLP Resources","uri":"/awesome-nlp-resources/"},{"categories":["Python Packages"],"content":" NLP360 is curated list of resources related to Natural Language Processing (NLP) : Datasets + Python packages and is updated frequently \r","date":"31/10/2020","objectID":"/awesome-nlp-resources/:0:0","tags":["Machine Learning","Python Packages","Awesome NLP"],"title":"NLP360 : Awesome NLP Resources","uri":"/awesome-nlp-resources/"},{"categories":["Python Packages"],"content":"NLP Datasets Complete NLP Dataset by The Eye - ArXiv (37GB), PubMed (6GB), StackExchange (34GB), OpenWebText (27GB), Github (106GB) The Big Bad NLP Database - Added the CommonCrawl datasets to the Big Bad NLP Database CommonCrawl by Facebook - Facebook release CommonCrawl dataset of 2.5TB of clean unsupervised text from 100 languages Wikipedia Data - CSV file containing the Wikidata id, title, lat/lng coordinates, and short description for all Wikipedia articles with location data (updated) Datasets by Transformers - Datasets and evaluation metrics for natural language processing by Transformers. Compatible with NumPy, Pandas, PyTorch and TensorFlow \r Multidomain Sentiment Analysis Dataset - This is a slightly older dataset that features a variety of product reviews taken from Amazon. IMDB Reviews - Featuring 25,000 movie reviews, this relatively small dataset was compiled primarily for binary sentiment classification use cases. Stanford Sentiment Treebank - Also built from movie reviews, Stanford‚Äôs dataset was designed to train a model to identify sentiment in longer phrases. It contains over 10,000 snippets taken from Rotten Tomatoes. Sentiment140 - This popular dataset contains 160,000 tweets formatted with 6 fields: polarity, ID, tweet date, query, user, and the text. Emoticons have been pre-removed. Twitter US Airline Sentiment - Scraped in February 2015, these tweets about US airlines are classified as classified as positive, negative, and neutral. Negative tweets have also been categorized by reason for complaint. \r 20 Newsgroups - This collection of approximately 20,000 documents covers 20 different newsgroups, from baseball to religion. ArXiv - This repository contains all of the arXiv research paper archive as fulltext, with a total dataset size of 270 GB. Reuters News Dataset - The documents in this dataset appeared on Reuters in 1987. They have since been assembled and indexed for use in machine learning. The WikiQA Corpus - This corpus is a publicly-available collection of question and answer pairs. It was originally assembled for use in research on open-domain question answering. UCI‚Äôs Spambase - Originally created by a team at Hewlett-Packard, this large spam email dataset is useful for developing personalized spam filters. Yelp Reviews - This open dataset released by Yelp contains more than 5 million reviews. WordNet - Compiled by researchers at Princeton University, WordNet is essentially a large lexical database of English ‚Äòsynsets‚Äô, or groups of synonyms that each describe a different, distinct concept. The Blog Authorship Corpus ‚Äì This dataset includes over 681,000 posts written by 19,320 different bloggers. In total, there are over 140 million words within the corpus. Enron Dataset - Over half a million anonymized emails from over 100 users. It‚Äôs one of the few publically available collections of ‚Äúreal‚Äù emails available for study and training sets. Project Gutenberg - Extensive collection of book texts. These are public domain and available in a variety of languages, spanning a long period of time. \r","date":"31/10/2020","objectID":"/awesome-nlp-resources/:1:0","tags":["Machine Learning","Python Packages","Awesome NLP"],"title":"NLP360 : Awesome NLP Resources","uri":"/awesome-nlp-resources/"},{"categories":["Python Packages"],"content":"NLP Python Packages Haystack - Open-source framework for building end-to-end question answering systems for large document collections. AdaptNLP - Powerful NLP toolkit built on top of Flair and Transformers for running, training and deploying state of the art deep learning models. Unified API for end to end NLP tasks: Token tagging, Text Classification, Question Anaswering, Embeddings, Translation, Text Generation etc. Sentence-Transformers - Python package to compute the dense vector representations of sentences or paragraphs using SOTA pretrained Transformers models. Tweet-Preprocessor - Python library to clean text/tweets in a single line of code. SimpleTransformers - Simple library to build any NLP deep learning models in 3 lines of code. It packs all the powerful features of Huggingface‚Äôs transformers in just 3 lines of code for end to end NLP tasks. TextAttack - Adversarial attacks, adversarial training, and data augmentation in NLP Fast.ai - Super high-level abstractions and easy implementations for NLP data preprocessing, model construction, training, and evaluation. TorchText - Convenient data processing utilities to process and prepare them in batches before you feed them into your deep learning framework OpenNMT - Convenient and powerful tool for the machine translation and sequence learning tasks ParlAI - Task-Oriented Dialogue, Chit-chat Dialogue, Visual Question Answering DeepPavlov - Framework mainly for chatbots and virtual assistants development, as it provides all the environment tools necessary for a production-ready and industry-grade conversational agent TextBlob - Providing a consistent API for diving into common natural language processing (NLP) tasks. Stands on the giant shoulders of Natural Language Toolkit (NLTK) and Pattern, and plays nicely with both :+1: spaCy - Industrial strength NLP with Python and Cython :+1: textacy - Higher level NLP built on spaCy gensim - Python library to conduct unsupervised semantic modelling from plain text :+1: scattertext - Python library to produce d3 visualizations of how language differs between corpora GluonNLP - A deep learning toolkit for NLP, built on MXNet/Gluon, for research prototyping and industrial deployment of state-of-the-art models on a wide range of NLP tasks. AllenNLP - An NLP research library, built on PyTorch, for developing state-of-the-art deep learning models on a wide variety of linguistic tasks. PyTorch-NLP - NLP research toolkit designed to support rapid prototyping with better data loaders, word vector loaders, neural network layer representations, common NLP metrics such as BLEU Rosetta - Text processing tools and wrappers (e.g. Vowpal Wabbit) PyNLPl - Python Natural Language Processing Library. General purpose NLP library for Python. Also contains some specific modules for parsing common NLP formats, most notably for FoLiA, but also ARPA language models, Moses phrasetables, GIZA++ alignments. PySS3 - Python package that implements a novel white-box machine learning model for text classification, called SS3. Since SS3 has the ability to visually explain its rationale, this package also comes with easy-to-use interactive visualizations tools (online demos). jPTDP - A toolkit for joint part-of-speech (POS) tagging and dependency parsing. jPTDP provides pre-trained models for 40+ languages. BigARTM - a fast library for topic modelling Snips NLU - A production ready library for intent parsing Chazutsu - A library for downloading\u0026parsing standard NLP research datasets Word Forms - Word forms can accurately generate all possible forms of an English word Multilingual Latent Dirichlet Allocation (LDA) - A multilingual and extensible document clustering pipeline NLP Architect - A library for exploring the state-of-the-art deep learning topologies and techniques for NLP and NLU Flair - A very simple framework for state-of-the-art multilingual NLP built on PyTorch. Includes BERT, ELMo and Flair embeddings. Kashgari - Simple, Keras-powered multilingual","date":"31/10/2020","objectID":"/awesome-nlp-resources/:2:0","tags":["Machine Learning","Python Packages","Awesome NLP"],"title":"NLP360 : Awesome NLP Resources","uri":"/awesome-nlp-resources/"},{"categories":["Python Packages"],"content":"Uber just open-sourced its time series modeling packagae - Orbit based on probabilistic modeling","date":"30/10/2020","objectID":"/orbit-uber-opensources-python-package-for-time-series-modeling/","tags":["Orbit","Machine Learning","Time Series Modeling","Python Packages"],"title":"Orbit | Uber's New Open Source Python Library for Time Series Modeling","uri":"/orbit-uber-opensources-python-package-for-time-series-modeling/"},{"categories":["Python Packages"],"content":"Orbit is Uber‚Äôs new python package for time series modeling and inference using Bayesian sampling methods for model estimation. Orbit provides a familiar and intuitive initialize-fit-predict interface for working with time series tasks, while utilizing probabilistic modeling under the hood. As per Orbit‚Äôs documentation, initial release supports concrete implementation for the following models: Local Global Trend (LGT) Damped Local Trend (DLT) Both models, which are variants of exponential smoothing, support seasonality and exogenous (time-independent) features. The initial release also supports the following sampling methods for model estimation: Markov-Chain Monte Carlo (MCMC) as a full sampling method Maximum a Posteriori (MAP) as a point estimate method Variational Inference (VI) as a hybrid-sampling method on approximate distribution ","date":"30/10/2020","objectID":"/orbit-uber-opensources-python-package-for-time-series-modeling/:0:0","tags":["Orbit","Machine Learning","Time Series Modeling","Python Packages"],"title":"Orbit | Uber's New Open Source Python Library for Time Series Modeling","uri":"/orbit-uber-opensources-python-package-for-time-series-modeling/"},{"categories":["Python Packages"],"content":"Quick Start Installation pip install orbit-ml Orbit requires PyStan as a system dependency. PyStan is licensed under GPLv3 , which is a free, copyleft license for software. Data iclaims_example is a dataset containing the weekly initial claims for US unemployment benefits against a few related google trend queries from Jan 2010 - June 2018. Number of claims are obtained from Federal Reserve Bank of St. Louis while google queries are obtained through Google Trends API. Quick Starter Code # import dependencies import pandas as pd import numpy as np import warnings from orbit.models.dlt import DLTMAP, DLTFull from orbit.diagnostics.plot import plot_predicted_data # Load data df = pd.read_csv(\"../../examples/data/iclaims_example.csv\", parse_dates=[\"week\"]) df[\"claims\"] = np.log(df[\"claims\"]) # train test split test_size = 104 train_df = df[:-test_size] test_df = df[-test_size:] # DLT model dlt = DLTMAP( response_col=\"claims\", date_col=\"week\", seasonality=52, seed=2020, ) # fit dlt.fit(df=train_df) # predict predicted_df = dlt.predict(df=test_df) # plot plot_predicted_data(training_actual_df=train_df, predicted_df=predicted_df, date_col=\"week\", actual_col=\"claims\", test_actual_df=test_df) ","date":"30/10/2020","objectID":"/orbit-uber-opensources-python-package-for-time-series-modeling/:1:0","tags":["Orbit","Machine Learning","Time Series Modeling","Python Packages"],"title":"Orbit | Uber's New Open Source Python Library for Time Series Modeling","uri":"/orbit-uber-opensources-python-package-for-time-series-modeling/"},{"categories":["Python Packages"],"content":"References Reference links\r\r Github ArXiv Paper Documentation \r\r","date":"30/10/2020","objectID":"/orbit-uber-opensources-python-package-for-time-series-modeling/:2:0","tags":["Orbit","Machine Learning","Time Series Modeling","Python Packages"],"title":"Orbit | Uber's New Open Source Python Library for Time Series Modeling","uri":"/orbit-uber-opensources-python-package-for-time-series-modeling/"},{"categories":["Python Packages"],"content":"Top 10 most useful but underrated python libraries for data science and machine learning","date":"18/10/2020","objectID":"/top-useful-but-underrated-python-libraries-for-data-science/","tags":["Data Science","Machine Learning","Python Packages"],"title":"Top 10 Most Useful But Underrated Python Libraries for Data¬†Science","uri":"/top-useful-but-underrated-python-libraries-for-data-science/"},{"categories":["Python Packages"],"content":"Introduction Python community offers jillion of python packages for data science pipeline starting from data cleaning to building deep learning models to deployment. Most appreciated and commonly used packages are‚Ää-‚Ää Pandas‚Ää: Data manipulation and analysis Matplotlib/Seaborn/Plotly‚Ää : Data visualization Scikit-learn‚Ää: Building Machine Learning models Keras/Tensorflow/Pytorch‚Ää: building deep learning models Flask‚Ää : Web app development/ML Applications These packages have received their appreciation and love from the data science community. But there are some python libraries in data science that are useful but underrated at the same time.¬†These packages can save you from writing a lot of code. They give you the ease of using state of the art models in one just single line of code. Let‚Äôs dive in. ","date":"18/10/2020","objectID":"/top-useful-but-underrated-python-libraries-for-data-science/:1:0","tags":["Data Science","Machine Learning","Python Packages"],"title":"Top 10 Most Useful But Underrated Python Libraries for Data¬†Science","uri":"/top-useful-but-underrated-python-libraries-for-data-science/"},{"categories":["Python Packages"],"content":"1. MissingNo MissingNo is a python library for null value or missing values analysis with impressive visualization like data display, bar charts, heatmaps and dendograms. Installation: pip install missingno Github: MissingNo # pip install missingno import missingno as msno # missing value visualization: dense data display msno.matrix(dataframe) # missing value visualization: bar charts msno.bar(dataframe) # missing value visualization: heatmaps msno.heatmap(dataframe) # missing value visualization: Dendogram msno.dendrogram(dataframe) ","date":"18/10/2020","objectID":"/top-useful-but-underrated-python-libraries-for-data-science/:1:1","tags":["Data Science","Machine Learning","Python Packages"],"title":"Top 10 Most Useful But Underrated Python Libraries for Data¬†Science","uri":"/top-useful-but-underrated-python-libraries-for-data-science/"},{"categories":["Python Packages"],"content":"2. MLExtend MLExtend stands for Machine Learning Extensions and is created by Sebastian Raschka. As the name suggests, it extends the current implementation of many machine learning algorithms, makes it more useful to use and definitely saves a lot of time. For e.g. Association Rule Mining (with Apriori, Fpgrowth \u0026 Fpmax support), EnsembleVoteClassifier, StackingCVClassifier Installation: pip install mlxtend Github: MlExtend For e.g. StackingCVClassifier can be implemented as from sklearn import model_selection from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.naive_bayes import GaussianNB from sklearn.ensemble import RandomForestClassifier from mlxtend.classifier import StackingCVClassifier import numpy as np import warnings warnings.simplefilter('ignore') RANDOM_SEED = 42 clf1 = KNeighborsClassifier(n_neighbors=1) clf2 = RandomForestClassifier(random_state=RANDOM_SEED) clf3 = GaussianNB() lr = LogisticRegression() # Starting from v0.16.0, StackingCVRegressor supports # `random_state` to get deterministic result. sclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3], meta_classifier=lr, random_state=RANDOM_SEED) print('3-fold cross validation:\\n') for clf, label in zip([clf1, clf2, clf3, sclf], ['KNN', 'Random Forest', 'Naive Bayes', 'StackingClassifier']): scores = model_selection.cross_val_score(clf, X, y, cv=3, scoring='accuracy') print(\"Accuracy: %0.2f(+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label)) ","date":"18/10/2020","objectID":"/top-useful-but-underrated-python-libraries-for-data-science/:1:2","tags":["Data Science","Machine Learning","Python Packages"],"title":"Top 10 Most Useful But Underrated Python Libraries for Data¬†Science","uri":"/top-useful-but-underrated-python-libraries-for-data-science/"},{"categories":["Python Packages"],"content":"3. Flair Flair is a powerful NLP library‚Ääwhich allows you to apply our state-of-the-art natural language processing (NLP) models to your text, such as named entity recognition (NER), part-of-speech tagging (PoS), sense disambiguation and classification. Installation: pip install flair Github: Flair Yes‚Ää- You have many libraries which promises that‚Ää- What sets Flair apart? It‚Äôs Stacked embeddings! Stacked embeddings is one of the most interesting features of Flair which will make you use this library even more. They provide means to combine different embeddings together. You can use both traditional word embeddings (like GloVe, word2vec, ELMo) together with Flair contextual string embeddings or BERT. You can very easily mix and match Flair, ELMo, BERT and classic word embeddings. All you need to do is instantiate each embedding you wish to combine and use them in a StackedEmbedding.‚ÄäFor instance, let‚Äôs say we want to combine the multilingual Flair and BERT embeddings to train a hyper-powerful multilingual downstream task model. First, instantiate the embeddings you wish to combine: from flair.embeddings import FlairEmbeddings, BertEmbeddings # init Flair embeddings flair_forward_embedding = FlairEmbeddings('multi-forward') flair_backward_embedding = FlairEmbeddings('multi-backward') # init multilingual BERT bert_embedding = BertEmbeddings('bert-base-multilingual-cased') from flair.embeddings import StackedEmbeddings # now create the StackedEmbedding object that combines all embeddings stacked_embeddings = StackedEmbeddings( embeddings=[flair_forward_embedding, flair_backward_embedding, bert_embedding]) Now just use this embedding like all the other embeddings, i.e. call the embed() method over your sentences. sentence = Sentence('The grass is green .') # just embed a sentence using the StackedEmbedding # as you would with any single embedding. stacked_embeddings.embed(sentence) # now check out the embedded tokens. for token in sentence: print(token) print(token.embedding) ","date":"18/10/2020","objectID":"/top-useful-but-underrated-python-libraries-for-data-science/:1:3","tags":["Data Science","Machine Learning","Python Packages"],"title":"Top 10 Most Useful But Underrated Python Libraries for Data¬†Science","uri":"/top-useful-but-underrated-python-libraries-for-data-science/"},{"categories":["Python Packages"],"content":"4. AdaptNLP AdaptNLP is another easy to use but powerful NLP toolkit built on top of Flair and Transformers for running, training and deploying state of the art deep learning models. It has a unified API for end to end NLP tasks: Token tagging, Text Classification, Question Anaswering, Embeddings, Translation, Text Generation etc. Installation: pip install adaptnlp Github: AdaptNLP Sample code for Question Answering from adaptnlp import EasyQuestionAnswering from pprint import pprint ## Example Query and Context query = \"What is the meaning of life?\" context = \"Machine Learning is the meaning of life.\" top_n = 5 ## Load the QA module and run inference on results qa = EasyQuestionAnswering() best_answer, best_n_answers = qa.predict_qa(query=query, context=context, n_best_size=top_n, mini_batch_size=1, model_name_or_path=\"distilbert-base-uncased-distilled-squad\") ## Output top answer as well as top 5 answers print(best_answer) pprint(best_n_answers) Sample code for Summarization from adaptnlp import EasySummarizer # Text from encyclopedia Britannica on Einstein text = \"\"\"Einstein would write that two ‚Äúwonders‚Äù deeply affected his early years. The first was his encounter with a compass at age five. He was mystified that invisible forces could deflect the needle. This would lead to a lifelong fascination with invisible forces. The second wonder came at age 12 when he discovered a book of geometry, which he devoured, calling it his 'sacred little geometry book'. Einstein became deeply religious at age 12, even composing several songs in praise of God and chanting religious songs on the way to school. This began to change, however, after he read science books that contradicted his religious beliefs. This challenge to established authority left a deep and lasting impression. At the Luitpold Gymnasium, Einstein often felt out of place and victimized by a Prussian-style educational system that seemed to stifle originality and creativity. One teacher even told him that he would never amount to anything.\"\"\" summarizer = EasySummarizer() # Summarize summaries = summarizer.summarize(text = text, model_name_or_path=\"t5-small\", mini_batch_size=1, num_beams = 4, min_length=0, max_length=100, early_stopping=True) print(\"Summaries:\\n\") for s in summaries: print(s, \"\\n\") ","date":"18/10/2020","objectID":"/top-useful-but-underrated-python-libraries-for-data-science/:1:4","tags":["Data Science","Machine Learning","Python Packages"],"title":"Top 10 Most Useful But Underrated Python Libraries for Data¬†Science","uri":"/top-useful-but-underrated-python-libraries-for-data-science/"},{"categories":["Python Packages"],"content":"5. SimpleTransformers SimpleTransformers is awesome and my go to library for any NLP deep learning models. It packs all the powerful features of Huggingface‚Äôs transformers in just 3 lines of code for end to end NLP tasks. Installation: pip install simpletransformers Github: SimpleTransformers Sample code for Text Classification from simpletransformers.classification import ClassificationModel import pandas as pd import logging logging.basicConfig(level=logging.INFO) transformers_logger = logging.getLogger(\"transformers\") transformers_logger.setLevel(logging.WARNING) # Train and Evaluation data needs to be in a Pandas Dataframe of two columns. The first column is the text with type str, and the second column is the label with type int. train_data = [['Example sentence belonging to class 1', 1], ['Example sentence belonging to class 0', 0]] train_df = pd.DataFrame(train_data) eval_data = [['Example eval sentence belonging to class 1', 1], ['Example eval sentence belonging to class 0', 0]] eval_df = pd.DataFrame(eval_data) # Create a ClassificationModel model = ClassificationModel('roberta', 'roberta-base') # You can set class weights by using the optional weight argument # Train the model model.train_model(train_df) # Evaluate the model result, model_outputs, wrong_predictions = model.eval_model(eval_df) Sample code for Language Model Training from simpletransformers.language_modeling import LanguageModelingModel import logging logging.basicConfig(level=logging.INFO) transformers_logger = logging.getLogger(\"transformers\") transformers_logger.setLevel(logging.WARNING) train_args = { \"reprocess_input_data\": True, \"overwrite_output_dir\": True, } model = LanguageModelingModel('bert', 'bert-base-cased', args=train_args) model.train_model(\"wikitext-2/wiki.train.tokens\", eval_file=\"wikitext-2/wiki.test.tokens\") model.eval_model(\"wikitext-2/wiki.test.tokens\") ","date":"18/10/2020","objectID":"/top-useful-but-underrated-python-libraries-for-data-science/:1:5","tags":["Data Science","Machine Learning","Python Packages"],"title":"Top 10 Most Useful But Underrated Python Libraries for Data¬†Science","uri":"/top-useful-but-underrated-python-libraries-for-data-science/"},{"categories":["Python Packages"],"content":"6. Sentence-Transformers Sentence-Transformers is a python package to compute the dense vector representations of sentences or paragraphs. Installation: pip install -U sentence-transformers Github: Sentence-Transformers This library not only allows to generate embeddings from state of the art pretrained transformer models but also allows embeddings after fine-tuning on your custom dataset. These embeddings are useful for various downstream tasks like semantic search or clustering Sample code for computing Embeddings from sentence_transformers import SentenceTransformer model = SentenceTransformer('distilbert-base-nli-mean-tokens') sentences = ['This framework generates embeddings for each input sentence', 'Sentences are passed as a list of string.', 'The quick brown fox jumps over the lazy dog.'] sentence_embeddings = model.encode(sentences) for sentence, embedding in zip(sentences, sentence_embeddings): print(\"Sentence:\", sentence) print(\"Embedding:\", embedding) print(\"\") Sample code for Semantic Search from sentence_transformers import SentenceTransformer, util import torch embedder = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens') # Corpus with example sentences corpus = ['A man is eating food.', 'A man is eating a piece of bread.', 'The girl is carrying a baby.', 'A man is riding a horse.', 'A woman is playing violin.', 'Two men pushed carts through the woods.', 'A man is riding a white horse on an enclosed ground.', 'A monkey is playing drums.', 'A cheetah is running behind its prey.' ] corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True) # Query sentences: queries = ['A man is eating pasta.', 'Someone in a gorilla costume is playing a set of drums.', 'A cheetah chases prey on across a field.'] # Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity top_k = 5 for query in queries: query_embedding = embedder.encode(query, convert_to_tensor=True) cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0] cos_scores = cos_scores.cpu() #We use torch.topk to find the highest 5 scores top_results = torch.topk(cos_scores, k=top_k) print(\"\\n\\n======================\\n\\n\") print(\"Query:\", query) print(\"\\nTop 5 most similar sentences in corpus:\") for score, idx in zip(top_results[0], top_results[1]): print(corpus[idx], \"(Score: %.4f)\" % (score)) ","date":"18/10/2020","objectID":"/top-useful-but-underrated-python-libraries-for-data-science/:1:6","tags":["Data Science","Machine Learning","Python Packages"],"title":"Top 10 Most Useful But Underrated Python Libraries for Data¬†Science","uri":"/top-useful-but-underrated-python-libraries-for-data-science/"},{"categories":["Python Packages"],"content":"7. Tweet-Preprocessor Preprocessing social data can be a bit frustrating at times bacause of irrelevant elements within the text like links, emojis, hashtags, usernames, mentions etc.. But not any more! Tweet-Preprocessor is for you. This library cleans your text/tweets in a single line of code. Installation: pip install tweet-preprocessor Github: Tweet-Preprocessor \u003e\u003e\u003e import preprocessor as p \u003e\u003e\u003e p.clean('Preprocessor is #awesome üëç https://github.com/s/preprocessor') # output: 'Preprocessor is' Currently supports cleaning, tokenizing and parsing URLs, Hashtags, Mentions, Reserved words (RT, FAV), Emojis, SmileysNumbers and you have full control over what you want to clean from the text. ","date":"18/10/2020","objectID":"/top-useful-but-underrated-python-libraries-for-data-science/:1:7","tags":["Data Science","Machine Learning","Python Packages"],"title":"Top 10 Most Useful But Underrated Python Libraries for Data¬†Science","uri":"/top-useful-but-underrated-python-libraries-for-data-science/"},{"categories":["Python Packages"],"content":"8. Gradio Gradio is another super cool library to quickly create customizable UI components to demo your ML/DL models within your jupyter notebook or in the browser. Installation: pip install gradio Github: Gradio Source: Gradio ","date":"18/10/2020","objectID":"/top-useful-but-underrated-python-libraries-for-data-science/:1:8","tags":["Data Science","Machine Learning","Python Packages"],"title":"Top 10 Most Useful But Underrated Python Libraries for Data¬†Science","uri":"/top-useful-but-underrated-python-libraries-for-data-science/"},{"categories":["Python Packages"],"content":"9. PPScore PPScore is Predictive Power Score which is a better alternative than df.corr() to find the correlation or relationship between 2 variables. Installation: pip install -U ppscore Github: PPScore Why is it better than correlation? It detects both linear and non-linear relationship between 2 columns It gives a normalized score ranging from 0 (no predictive power) to 1 (perfect predictive power) It takes both numeric and categorical variables as input, so no need to convert your categorical variables into dummy variables before feeding it to PPScore. import ppscore as pps # Based on the dataframe we can calculate the PPS of x predicting y pps.score(df, \"x\", \"y\") # We can calculate the PPS of all the predictors in the dataframe against a target y pps.predictors(df, \"y\") ","date":"18/10/2020","objectID":"/top-useful-but-underrated-python-libraries-for-data-science/:1:9","tags":["Data Science","Machine Learning","Python Packages"],"title":"Top 10 Most Useful But Underrated Python Libraries for Data¬†Science","uri":"/top-useful-but-underrated-python-libraries-for-data-science/"},{"categories":["Python Packages"],"content":"10. Pytorch-Forecasting Pytorch-Forecasting is python toolkit built on top of pytorch-lightening which aims to solve time series forecasting with neural networks with ease. Installation: pip install pytorch-forecasting Github: Pytorch-Forecasting This library provides abstraction over handling missing values, variable transformation, Tensorboard support, prediction \u0026 dependency plots, Range optimizer for faster training and Optuna for hyperparamter tuning. Usage script: Pytorch-Forecasting import pytorch_lightning as pl from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer # load data data = ... # define dataset max_encode_length = 36 max_prediction_length = 6 training_cutoff = \"YYYY-MM-DD\" # day for cutoff training = TimeSeriesDataSet( data[lambda x: x.date \u003c= training_cutoff], time_idx= ..., target= ..., group_ids=[ ... ], max_encode_length=max_encode_length, max_prediction_length=max_prediction_length, static_categoricals=[ ... ], static_reals=[ ... ], time_varying_known_categoricals=[ ... ], time_varying_known_reals=[ ... ], time_varying_unknown_categoricals=[ ... ], time_varying_unknown_reals=[ ... ], ) validation = TimeSeriesDataSet.from_dataset(training, data, min_prediction_idx=training.index.time.max() + 1, stop_randomization=True) batch_size = 128 train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=2) val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=2) early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=1, verbose=False, mode=\"min\") lr_logger = LearningRateMonitor() trainer = pl.Trainer( max_epochs=100, gpus=0, gradient_clip_val=0.1, limit_train_batches=30, callbacks=[lr_logger, early_stop_callback], ) tft = TemporalFusionTransformer.from_dataset( training, learning_rate=0.03, hidden_size=32, attention_head_size=1, dropout=0.1, hidden_continuous_size=16, output_size=7, loss=QuantileLoss(), log_interval=2, reduce_on_plateau_patience=4 ) print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\") # find optimal learning rate res = trainer.lr_find( tft, train_dataloader=train_dataloader, val_dataloaders=val_dataloader, early_stop_threshold=1000.0, max_lr=0.3, ) print(f\"suggested learning rate: {res.suggestion()}\") fig = res.plot(show=True, suggest=True) fig.show() trainer.fit( tft, train_dataloader=train_dataloader, val_dataloaders=val_dataloader, ) ","date":"18/10/2020","objectID":"/top-useful-but-underrated-python-libraries-for-data-science/:1:10","tags":["Data Science","Machine Learning","Python Packages"],"title":"Top 10 Most Useful But Underrated Python Libraries for Data¬†Science","uri":"/top-useful-but-underrated-python-libraries-for-data-science/"},{"categories":["Natural Language Understanding"],"content":"This article is a step by step guide to build a faster and accurate COVID Semantic Search Engine using HuggingFace Transformersü§ó. In this article, we will build a search engine, which will not only retrieve and rank the articles based on the query but also give us the response, along with a 1000 words context around the response","date":"14/10/2020","objectID":"/building-a-faster-accurate-covid-search-engine-with-transformers/","tags":["Deep Learning","Transformers","Semantic Search","QA Model"],"title":"Building A Faster \u0026 Accurate COVID Search Engine with Transformersü§ó","uri":"/building-a-faster-accurate-covid-search-engine-with-transformers/"},{"categories":["Natural Language Understanding"],"content":"Abstract\r\rThis article will let you build a faster and accurate COVID Search Engine using Transformersü§ó\r\r Image by Gerd Altmann from Pixabay ","date":"14/10/2020","objectID":"/building-a-faster-accurate-covid-search-engine-with-transformers/:0:0","tags":["Deep Learning","Transformers","Semantic Search","QA Model"],"title":"Building A Faster \u0026 Accurate COVID Search Engine with Transformersü§ó","uri":"/building-a-faster-accurate-covid-search-engine-with-transformers/"},{"categories":["Natural Language Understanding"],"content":"Introduction In this article, we will build a search engine, which will not only retrieve and rank the articles based on the query but also give us the response, along with a 1000 words context around the response. To achieve this, we will need: a structured dataset with reserach papers and its full text. Transformers library to build QA model and Finally, Haystack library to scale QA model to thousands of documents and build a search engine. ","date":"14/10/2020","objectID":"/building-a-faster-accurate-covid-search-engine-with-transformers/:1:0","tags":["Deep Learning","Transformers","Semantic Search","QA Model"],"title":"Building A Faster \u0026 Accurate COVID Search Engine with Transformersü§ó","uri":"/building-a-faster-accurate-covid-search-engine-with-transformers/"},{"categories":["Natural Language Understanding"],"content":"Data For this tutorial, we will use COVID-19 Open Research Dataset Challenge (CORD-19). CORD-19 is a resource of over 200,000 scholarly articles, including over 100,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. This dataset is ideal for building document retrieval system as it has full research paper content in text format. Columns like paper_id: Unique identifier of research paper title: title of research paper abstract: Bried summary of the research paper full_text: Full text/content of the research paper; are of our interest. In Kaggle Folder Structure - There are 2 directories - pmc_json and pdf_json - which contains the data in the json format. We will take 25,000 articles from pmc_json directory and 25000 articles from pdf_json - So, a total of 50,000 research articles to build our search engine. We will extract paper_id, title, abstract, full_text and put it in an easy to use pandas.DataFrame. ","date":"14/10/2020","objectID":"/building-a-faster-accurate-covid-search-engine-with-transformers/:2:0","tags":["Deep Learning","Transformers","Semantic Search","QA Model"],"title":"Building A Faster \u0026 Accurate COVID Search Engine with Transformersü§ó","uri":"/building-a-faster-accurate-covid-search-engine-with-transformers/"},{"categories":["Natural Language Understanding"],"content":"Let‚Äôs Code Note\r\rWe will use Kaggle notebook to write our code to leverage GPU.\r\r ","date":"14/10/2020","objectID":"/building-a-faster-accurate-covid-search-engine-with-transformers/:3:0","tags":["Deep Learning","Transformers","Semantic Search","QA Model"],"title":"Building A Faster \u0026 Accurate COVID Search Engine with Transformersü§ó","uri":"/building-a-faster-accurate-covid-search-engine-with-transformers/"},{"categories":["Natural Language Understanding"],"content":"Load the data import numpy as np import pandas as pd import os import json import re from tqdm import tqdm dirs=[\"pmc_json\",\"pdf_json\"] docs=[] counts=0 for d in dirs: print(d) counts = 0 for file in tqdm(os.listdir(f\"../input/CORD-19-research-challenge/document_parses/{d}\")):#What is an f string? file_path = f\"../input/CORD-19-research-challenge/document_parses/{d}/{file}\" j = json.load(open(file_path,\"rb\")) #Taking last 7 characters. it removes the 'PMC' appended to the beginning #also paperid in pdf_json are guids and hard to plot in the graphs hence the substring paper_id = j['paper_id'] paper_id = paper_id[-7:] title = j['metadata']['title'] try:#sometimes there are no abstracts abstract = j['abstract'][0]['text'] except: abstract = \"\" full_text = \"\" bib_entries = [] for text in j['body_text']: full_text += text['text'] docs.append([paper_id, title, abstract, full_text]) #comment this below block if you want to consider all files #comment block start counts = counts + 1 if(counts \u003e= 25000): break #comment block end df=pd.DataFrame(docs,columns=['paper_id','title','abstract','full_text']) print(df.shape) df.head() pmc_json 34%|‚ñà‚ñà‚ñà‚ñé | 24999/74137 [01:29\u003c02:56, 278.06it/s] pdf_json 25%|‚ñà‚ñà‚ñç | 24999/100423 [01:24\u003c04:15, 295.09it/s] (50000, 4) We have 50,000 articles and columns like paper_id, title, abstract and full_text We will be interested in title and full_text columns as these columns will be used to build the engine. Let‚Äôs setup a Search Engine on top full_text - which contains the full content of the research papers. ","date":"14/10/2020","objectID":"/building-a-faster-accurate-covid-search-engine-with-transformers/:3:1","tags":["Deep Learning","Transformers","Semantic Search","QA Model"],"title":"Building A Faster \u0026 Accurate COVID Search Engine with Transformersü§ó","uri":"/building-a-faster-accurate-covid-search-engine-with-transformers/"},{"categories":["Natural Language Understanding"],"content":"Haystack Now, Welcome Haystack! The secret sauce behind setting up a search engine and ability to scale any QA model to thousands of documents. Haystack helps you scale QA models to large collections of documents! You can read more about this amazing library here https://github.com/deepset-ai/haystack For installation: ! pip install git+https://github.com/deepset-ai/haystack.git But just to give a background, there are 3 major components to Haystack. Document Store: Database storing the documents for our search. We recommend Elasticsearch, but have also more light-weight options for fast prototyping (SQL or In-Memory). Retriever: Fast, simple algorithm that identifies candidate passages from a large collection of documents. Algorithms include TF-IDF or BM25, custom Elasticsearch queries, and embedding-based approaches. The Retriever helps to narrow down the scope for Reader to smaller units of text where a given question could be answered. Reader: Powerful neural model that reads through texts in detail to find an answer. Use diverse models like BERT, RoBERTa or XLNet trained via FARM or Transformers on SQuAD like tasks. The Reader takes multiple passages of text as input and returns top-n answers with corresponding confidence scores. You can just load a pretrained model from Hugging Face‚Äôs model hub or fine-tune it to your own domain data. And then there is Finder which glues together a Reader and a Retriever as a pipeline to provide an easy-to-use question answering interface. Now, we can setup Haystack in 3 steps: Install haystack and import its required modules Setup DocumentStore Setup Retriever, Reader and Finder ","date":"14/10/2020","objectID":"/building-a-faster-accurate-covid-search-engine-with-transformers/:3:2","tags":["Deep Learning","Transformers","Semantic Search","QA Model"],"title":"Building A Faster \u0026 Accurate COVID Search Engine with Transformersü§ó","uri":"/building-a-faster-accurate-covid-search-engine-with-transformers/"},{"categories":["Natural Language Understanding"],"content":"1. Install haystack Let‚Äôs install haystack and import all the required modules # installing haystack ! pip install git+https://github.com/deepset-ai/haystack.git # importing necessary dependencies from haystack import Finder from haystack.indexing.cleaning import clean_wiki_text from haystack.indexing.utils import convert_files_to_dicts, fetch_archive_from_http from haystack.reader.farm import FARMReader from haystack.reader.transformers import TransformersReader from haystack.utils import print_answers ","date":"14/10/2020","objectID":"/building-a-faster-accurate-covid-search-engine-with-transformers/:3:3","tags":["Deep Learning","Transformers","Semantic Search","QA Model"],"title":"Building A Faster \u0026 Accurate COVID Search Engine with Transformersü§ó","uri":"/building-a-faster-accurate-covid-search-engine-with-transformers/"},{"categories":["Natural Language Understanding"],"content":"2. Setting up DocumentStore Haystack finds answers to queries within the documents stored in a DocumentStore. The current implementations of DocumentStore include ElasticsearchDocumentStore, SQLDocumentStore, and InMemoryDocumentStore. But they recommend ElasticsearchDocumentStore because as it comes preloaded with features like full-text queries, BM25 retrieval, and vector storage for text embeddings. So - Let‚Äôs set up a ElasticsearchDocumentStore. ! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q ! tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz ! chown -R daemon:daemon elasticsearch-7.6.2 import os from subprocess import Popen, PIPE, STDOUT es_server = Popen(['elasticsearch-7.6.2/bin/elasticsearch'], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1) # as daemon ) # wait until ES has started ! sleep 30 # initiating ElasticSearch from haystack.database.elasticsearch import ElasticsearchDocumentStore document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\") Once ElasticsearchDocumentStore is setup, we will write our documents/texts to the DocumentStore. Writing documents to ElasticsearchDocumentStore requires a format - List of dictionaries as shown below: [ {\"name\": \"\u003csome-document-name\u003e, \"text\": \"\u003cthe-actual-text\u003e\"}, {\"name\": \"\u003csome-document-name\u003e, \"text\": \"\u003cthe-actual-text\u003e\"} {\"name\": \"\u003csome-document-name\u003e, \"text\": \"\u003cthe-actual-text\u003e\"} ] (Optionally: you can also add more key-value-pairs here, that will be indexed as fields in Elasticsearch and can be accessed later for filtering or shown in the responses of the Finder) We will use title column to pass as name and full_text column to pass as the text # Now, let's write the dicts containing documents to our DB. document_store.write_documents(data[['title', 'abstract']].rename(columns={'title':'name','full_text':'text'}).to_dict(orient='records')) ","date":"14/10/2020","objectID":"/building-a-faster-accurate-covid-search-engine-with-transformers/:3:4","tags":["Deep Learning","Transformers","Semantic Search","QA Model"],"title":"Building A Faster \u0026 Accurate COVID Search Engine with Transformersü§ó","uri":"/building-a-faster-accurate-covid-search-engine-with-transformers/"},{"categories":["Natural Language Understanding"],"content":"3. Setup Retriever, Reader and Finder Retrievers help narrowing down the scope for the Reader to smaller units of text where a given question could be answered. They use some simple but fast algorithm. Here: We use Elasticsearch‚Äôs default BM25 algorithm from haystack.retriever.sparse import ElasticsearchRetriever retriever = ElasticsearchRetriever(document_store=document_store) A Reader scans the texts returned by retrievers in detail and extracts the k best answers. They are based on powerful, but slower deep learning models. Haystack currently supports Readers based on the frameworks FARM and Transformers. With both you can either load a local model or one from Hugging Face's model hub (https://huggingface.co/models). Here: a medium sized RoBERTa QA model using a Reader based on FARM (https://huggingface.co/deepset/roberta-base-squad2) reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True, context_window_size=500) Downloading: 100% 1.26k/1.26k [00:33\u003c00:00, 37.9B/s] Downloading: 100% 499M/499M [00:22\u003c00:00, 21.8MB/s] Downloading: 100% 899k/899k [00:03\u003c00:00, 272kB/s] Downloading: 100% 456k/456k [00:01\u003c00:00, 252kB/s] Downloading: 100% 150/150 [00:01\u003c00:00, 97.1B/s] Downloading: 100% 190/190 [00:00\u003c00:00, 342B/s] And finally: The Finder sticks together reader and retriever in a pipeline to fetch answers based on our query. finder = Finder(reader, retriever) ","date":"14/10/2020","objectID":"/building-a-faster-accurate-covid-search-engine-with-transformers/:3:5","tags":["Deep Learning","Transformers","Semantic Search","QA Model"],"title":"Building A Faster \u0026 Accurate COVID Search Engine with Transformersü§ó","uri":"/building-a-faster-accurate-covid-search-engine-with-transformers/"},{"categories":["Natural Language Understanding"],"content":"ü•≥ Voila! We‚Äôre Done. Let‚Äôs see, how well our search engine works! - For simplicity, we will keep the number of documents to be retrieved to 2 using top_k_reader parameter. But we can extend to any number in production. Now, whenever we search or query our DocumentStore, we get 3 responses- we get the answer a 1000 words context around the answer and the name/title of the research paper Example 1: What is the impact of coronavirus on babies? question = \"What is the impact of coronavirus on babies?\" number_of_answers_to_fetch = 2 prediction = finder.get_answers(question=question, top_k_retriever=10, top_k_reader=number_of_answers_to_fetch) print(f\"Question: {prediction['question']}\") print(\"\\n\") for i in range(number_of_answers_to_fetch): print(f\"#{i+1}\") print(f\"Answer: {prediction['answers'][i]['answer']}\") print(f\"Research Paper: {prediction['answers'][i]['meta']['name']}\") print(f\"Context: {prediction['answers'][i]['context']}\") print('\\n\\n') Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.17 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.83 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.71 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.75 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.78 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.83 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01\u003c00:00, 1.08s/ Batches] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01\u003c00:00, 1.09s/ Batches] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.83 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.57 Batches/s] Question: What is the impact of coronavirus on babies? #1 Answer: While babies have been infected, the naivete of the neonatal immune system in relation to the inflammatory response would appear to be protective, with further inflammatory responses achieved with the consumption of human milk. Research Paper: COVID 19 in babies: Knowledge for neonatal care Context:\rance to minimize the health systems impact of this pandemic across the lifespan.The Covid-19 pandemic has presented neonatal nurses and midwives with challenges when caring for mother and babies. This review has presented what is currently known aboutCovid-19 and neonatal health, and information and research as they are generated will add to a complete picture of the health outcomes. While babies have been infected, the naivete of the neonatal immune system in relation to the inflammatory response would appear to be protective, with further inflammatory responses achieved with the consumption of human milk. The WHO has made clear recommendations about the benefits of breastfeeding, even if the mother and baby dyad is Covid-19 positive, if they remain well. The mother and baby should not be separated, and the mother needs to be able to participate in her baby's care and develop her mothering role. The complexities of not being able to access her usual support people mean that the mother #2 Answer: neonate are mild, with low-grade fever and gastrointestinal signs such as poor feeding and vomiting. The respiratory symptoms are also limited to mild tachypnoea and/or tachycardia. Research Paper: COVID 19 in babies: Knowledge for neonatal care Context: Likewise, if the mother and baby are well, skin-to-skin and breast feeding should be encouraged, as the benefits outweigh any potential harms. If a neonate becomes unwell and requires intensive care, they should be nursed with droplet precautions in a closed incubator in a negative pressure room. The management is dictated by the presenting signs and symptoms. It would appear the presenting symptoms in the neonate are mild, with low-grade fever and gastrointestinal signs such as poor feeding and vomiting. The respiratory symptoms are also limited to mild tachypnoea and/or tachycardia. However, as there has been a presentation of seizure activity with fever a neurological examina","date":"14/10/2020","objectID":"/building-a-faster-accurate-covid-search-engine-with-transformers/:3:6","tags":["Deep Learning","Transformers","Semantic Search","QA Model"],"title":"Building A Faster \u0026 Accurate COVID Search Engine with Transformersü§ó","uri":"/building-a-faster-accurate-covid-search-engine-with-transformers/"},{"categories":["Natural Language Understanding"],"content":"Notebooks Attachments\r\r Go to Dataset Go to Published Kaggle Kernel \r\r","date":"14/10/2020","objectID":"/building-a-faster-accurate-covid-search-engine-with-transformers/:4:0","tags":["Deep Learning","Transformers","Semantic Search","QA Model"],"title":"Building A Faster \u0026 Accurate COVID Search Engine with Transformersü§ó","uri":"/building-a-faster-accurate-covid-search-engine-with-transformers/"},{"categories":["Text Classification"],"content":"In this article, we will see how to fine tune a XLNet model on custom data, for text classification using Transformersü§ó. XLNet is powerful! It beats BERT and its other variants in 20 different tasks. In simple words - XLNet is a generalized autoregressive model.\nAn Autoregressive model is a model which uses the context word to predict the next word. So, the next token is dependent on all previous tokens.\nXLNET is generalized because it captures bi-directional context by means of a mechanism called permutation language modeling.\nIt integrates the idea of auto-regressive models and bi-directional context modeling, yet overcoming the disadvantages of BERT and thus outperforming BERT on 20 tasks, often by a large margin in tasks such as question answering, natural language inference, sentiment analysis, and document ranking.\n\nIn this article, we will take a pretrained `XLNet` model and fine tune it on our dataset.","date":"13/10/2020","objectID":"/fine-tuning-xlnet-model-for-text-classification/","tags":["Deep Learning","Transformers","XLNet","Text Classification"],"title":"Fine Tuning XLNet Model for Text Classification","uri":"/fine-tuning-xlnet-model-for-text-classification/"},{"categories":["Text Classification"],"content":"Abstract\r\rIn this article, we will see how to fine tune a XLNet model on custom data, for text classification using Transformersü§ó \r\r ","date":"13/10/2020","objectID":"/fine-tuning-xlnet-model-for-text-classification/:0:0","tags":["Deep Learning","Transformers","XLNet","Text Classification"],"title":"Fine Tuning XLNet Model for Text Classification","uri":"/fine-tuning-xlnet-model-for-text-classification/"},{"categories":["Text Classification"],"content":"Introduction XLNet is powerful! It beats BERT and its other variants in 20 different tasks. The XLNet model was proposed in XLNet: Generalized Autoregressive Pretraining for Language Understanding by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le. XLnet is an extension of the Transformer-XL model pre-trained using an autoregressive method to learn bidirectional contexts by maximizing the expected likelihood over all permutations of the input sequence factorization order. In simple words - XLNet is a generalized autoregressive model. An Autoregressive model is a model which uses the context word to predict the next word. So, the next token is dependent on all previous tokens. XLNET is generalized because it captures bi-directional context by means of a mechanism called ‚Äúpermutation language modeling‚Äù. It integrates the idea of auto-regressive models and bi-directional context modeling, yet overcoming the disadvantages of BERT and thus outperforming BERT on 20 tasks, often by a large margin in tasks such as question answering, natural language inference, sentiment analysis, and document ranking. In this article, we will take a pretrained XLNet model and fine tune it on our dataset. So, let‚Äôs talk about the dataset. ","date":"13/10/2020","objectID":"/fine-tuning-xlnet-model-for-text-classification/:1:0","tags":["Deep Learning","Transformers","XLNet","Text Classification"],"title":"Fine Tuning XLNet Model for Text Classification","uri":"/fine-tuning-xlnet-model-for-text-classification/"},{"categories":["Text Classification"],"content":"Data We will take a dataset from Kaggle‚Äôs text classification challenge (Ongoing as of now) - Real or Not? NLP with Disaster Tweets. In this competition, we have to build a machine learning model that predicts which Tweets are about real disasters and which one‚Äôs aren‚Äôt. It‚Äôs a small dataset of 10,000 tweets that were hand classified. We will use this data to fine tune a pretrained XLNet model. ","date":"13/10/2020","objectID":"/fine-tuning-xlnet-model-for-text-classification/:2:0","tags":["Deep Learning","Transformers","XLNet","Text Classification"],"title":"Fine Tuning XLNet Model for Text Classification","uri":"/fine-tuning-xlnet-model-for-text-classification/"},{"categories":["Text Classification"],"content":"Let‚Äôs Code Note\r\rWe will use Colab notebook to write our code so that we can leverage GPU enabled environment.\r\r ","date":"13/10/2020","objectID":"/fine-tuning-xlnet-model-for-text-classification/:3:0","tags":["Deep Learning","Transformers","XLNet","Text Classification"],"title":"Fine Tuning XLNet Model for Text Classification","uri":"/fine-tuning-xlnet-model-for-text-classification/"},{"categories":["Text Classification"],"content":"Installing Dependencies First, lets spin up a Colab notebook. Download the data from Real or Not? NLP with Disaster Tweets. You will have 3 files, train.csv, test.csv and sample_submission.csv Upload it to your Colab Notebook session. Install the latest stable pytorch 1.6, transformers and simpletransformers. ! pip uninstall torch torchvision -y ! pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html !pip install -U transformers !pip install -U simpletransformers Now we‚Äôre good to go. ","date":"13/10/2020","objectID":"/fine-tuning-xlnet-model-for-text-classification/:3:1","tags":["Deep Learning","Transformers","XLNet","Text Classification"],"title":"Fine Tuning XLNet Model for Text Classification","uri":"/fine-tuning-xlnet-model-for-text-classification/"},{"categories":["Text Classification"],"content":"Preprocessing import pandas as pd import numpy as np df_train = pd.read_csv(\"train.csv\") df_test = pd.read_csv(\"test.csv\") df_train.head() We have 5 columns in our data: id: it is a unique identifier of tweets. keyword: It contains the keywords made on the tweets. location: The location the tweet was sent from. text: it is actual tweet made by the users target: Whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0. Let‚Äôs look at the distribution of target class df_train.target.value_counts() 0 4342 1 3271 Name: target, dtype: int64 The dataset is pretty much balanced. We have 3271 tweets about disasters while 4342 tweets otherwise. Let‚Äôs have a look at the keyword and location columns print(f\"Keyword column has {df_train.keyword.isnull().sum()/df_train.shape[0]*100}% null values\") print(f\"Location column has {df_train.location.isnull().sum()/df_train.shape[0]*100}% null values) Keyword column has 0.80% null values Location column has 33.27% null values location has 33% missing values while keyword has 0.8% null values. We will not delve into filling up missing values and will leave these columns as it is. The text and target columns is of our interest. Let‚Äôs have a look at the text column df_train.sample(10)['text'].tolist() ['Two giant cranes holding a bridge collapse into nearby homes http://t.co/jBJRg3eP1Q', \"Apollo Brown - 'Detonate' f. M.O.P. | http://t.co/H1xiGcEn7F\", 'Listening to Blowers and Tuffers on the Aussie batting collapse at Trent Bridge reminds me why I love @bbctms! Wonderful stuff! #ENGvAUS', 'Downtown Emergency Service Center is hiring! #Chemical #Dependency Counselor or Intern in #Seattle apply now! #jobs http://t.co/HhTwAyT4yo', 'Car engulfed in flames backs up traffic at Parley\\x89√õ¬™s Summit http://t.co/RmucfjCaZr', 'After death of Palestinian toddler in arson\\nattack Israel cracks down on Jewish', 'Students at Sutherland remember Australian casualties at Lone Pine Gallipoli\\n http://t.co/d50oRfXoFB via @theleadernews', 'FedEx no longer to transport bioterror germs in wake of anthrax lab mishaps http://t.co/hrqCJdovJZ', '@newyorkcity for the #international emergency medicine conference w/ Lennox Hill hospital and #drjustinmazur', 'My back is so sunburned :('] We see that the text columns contains #, @, and links which needs to be cleaned. Let‚Äôs write a simple function to clean up: # username starting with @ links We will use tweet-preprocessor to do this. tweet-preprocessor.clean() function can help us get rid of irrelevant tokens such as any hashtags, @username or links from the tweet and make it super clean to feed into XLNet model. ! pip install tweet-preprocessor import preprocessor as p from tqdm.notebook import tqdm tqdm.pandas() # function to clean @, #, and links from tweets def clean_text(text): text = text.replace(\"#\",\"\") return p.clean(text) # Appling function to train and test data from tqdm.notebook import tqdm tqdm.pandas() df_train['clean_text'] = df_train['text'].astype(str).progress_map(clean_text) df_test['clean_text'] = df_test['text'].astype(str).progress_map(clean_text) 100% 7613/7613 [00:49\u003c00:00, 154.19it/s] 100% 3263/3263 [00:48\u003c00:00, 67.34it/s] Now, we have clean text in clean_text column. Now, let‚Äôs split our data into train and eval set # splitting the data into training and eval dataset X = df_train['clean_text'] y = df_train['target'] from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) train_df = pd.DataFrame(X_train) train_df['target'] = y_train eval_df = pd.DataFrame(X_test) eval_df['target'] = y_test train_df.shape, eval_df.shape ((6090, 2), (1523, 2)) We divided our data into train_df and eval_df in 80:20 startified split. We have 6090 tweets for training and 1523 tweets for evaluation. Now, we are all set for training XLNet. ","date":"13/10/2020","objectID":"/fine-tuning-xlnet-model-for-text-classification/:3:2","tags":["Deep Learning","Transformers","XLNet","Text Classification"],"title":"Fine Tuning XLNet Model for Text Classification","uri":"/fine-tuning-xlnet-model-for-text-classification/"},{"categories":["Text Classification"],"content":"XLNet Training For training XLNet, we will use simpletransformers which is super easy to use library built on top of our beloved transformers. simpletransformers has a unified functions to train any SOTA pretrained NLP model available in transformers. So you get the power of SOTA pretrained language models like BERT and its variants, XLNet, ELECTRA, T5 etc. wrapped in easy to use functions. As you see below, it just takes 3 lines of code to train a XLNet model. And the same holds true for training it from scratch or just fine tuning the model on custom dataset. I have kept num_train_epochs: 4, train_batch_size: 32 and max_seq_length: 128 - so that it fits into Colab compute limits. Feel free to play with a lot of parameters mentioned in args in the code below. # We will import ClassificationModel - as we need to solve binary text classification from simpletransformers.classification import ClassificationModel import pandas as pd import logging import sklearn logging.basicConfig(level=logging.INFO) transformers_logger = logging.getLogger(\"transformers\") transformers_logger.setLevel(logging.WARNING) # They are lot of arguments to play with ''' args = { 'output_dir': 'outputs/', 'cache_dir': 'cache/', 'fp16': True, 'fp16_opt_level': 'O1', 'max_seq_length': 256, 'train_batch_size': 8, 'eval_batch_size': 8, 'gradient_accumulation_steps': 1, 'num_train_epochs': 3, 'weight_decay': 0, 'learning_rate': 4e-5, 'adam_epsilon': 1e-8, 'warmup_ratio': 0.06, 'warmup_steps': 0, 'max_grad_norm': 1.0, 'logging_steps': 50, 'evaluate_during_training': False, 'save_steps': 2000, 'eval_all_checkpoints': True, 'use_tensorboard': True, 'overwrite_output_dir': True, 'reprocess_input_data': False, } ''' # Create a ClassificationModel model = ClassificationModel('xlnet', 'xlnet-base-cased', args={'num_train_epochs':4, 'train_batch_size':32, 'max_seq_length':128}) # You can set class weights by using the optional weight argument # Train the model model.train_model(train_df) # Evaluate the model result, model_outputs, wrong_predictions = model.eval_model(eval_df, acc=sklearn.metrics.accuracy_score) Downloading: 100%\r760/760 [00:10\u003c00:00, 71.0B/s] Downloading: 100%\r467M/467M [00:10\u003c00:00, 45.2MB/s] Downloading: 100%\r798k/798k [00:14\u003c00:00, 56.1kB/s] 100%\r6090/6090 [08:15\u003c00:00, 12.29it/s] Epoch 4 of 4: 100%\r4/4 [08:12\u003c00:00, 123.24s/it] Epochs 0/4. Running Loss: 0.4059: 100%\r191/191 [08:12\u003c00:00, 2.58s/it] Epochs 1/4. Running Loss: 0.2305: 100%\r191/191 [02:01\u003c00:00, 1.57it/s] Epochs 2/4. Running Loss: 0.4360: 100%\r191/191 [04:24\u003c00:00, 1.38s/it] Epochs 3/4. Running Loss: 0.0260: 100%\r191/191 [02:28\u003c00:00, 1.28it/s] 100%\r1523/1523 [00:23\u003c00:00, 65.14it/s] Running Evaluation: 100%\r191/191 [00:20\u003c00:00, 9.17it/s] INFO:simpletransformers.classification.classification_model:{'mcc': 0.6457675302369492, 'tp': 518, 'tn': 741, 'fp': 128, 'fn': 136, 'acc': 0.8266579120157583, 'eval_loss': 0.5341164009543184} We have achieved a decent accuracy of 82.6% on our eval set. This accracy is just out of the box - means with no feature engineering, with no hyparameter-tuning. Just out of the box! ","date":"13/10/2020","objectID":"/fine-tuning-xlnet-model-for-text-classification/:3:3","tags":["Deep Learning","Transformers","XLNet","Text Classification"],"title":"Fine Tuning XLNet Model for Text Classification","uri":"/fine-tuning-xlnet-model-for-text-classification/"},{"categories":["Text Classification"],"content":"ü•≥ We‚Äôre Done! Let‚Äôs submit the predictions to Kaggle and see where we stand. predictions, raw_outputs = model.predict(df_test.clean_text.tolist()) sample_sub=pd.read_csv(\"sample_submission.csv\") sample_sub['target'] = predictions sample_sub.to_csv(\"submission_09092020_xlnet_base.csv\", index=False) INFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used. 100%\r3263/3263 [00:01\u003c00:00, 3216.81it/s] 100% 408/408 [00:38\u003c00:00, 10.68it/s] We‚Äôre in top 18%. It‚Äôs a good start considering XLNet out of the box performance - with no feature engineering at all. Now, we have a decent baseline to improve our model upon. ","date":"13/10/2020","objectID":"/fine-tuning-xlnet-model-for-text-classification/:4:0","tags":["Deep Learning","Transformers","XLNet","Text Classification"],"title":"Fine Tuning XLNet Model for Text Classification","uri":"/fine-tuning-xlnet-model-for-text-classification/"},{"categories":["Text Classification"],"content":"Notebooks Attachments\r\r Go to Dataset. Go to Google Colab Notebook \r\r","date":"13/10/2020","objectID":"/fine-tuning-xlnet-model-for-text-classification/:5:0","tags":["Deep Learning","Transformers","XLNet","Text Classification"],"title":"Fine Tuning XLNet Model for Text Classification","uri":"/fine-tuning-xlnet-model-for-text-classification/"},{"categories":["Natural Language Understanding"],"content":"In this article, you will learn how to fetch contextual answers in a huge corpus of documents using Transformersü§ó. We will build a neural question and answering system using transformers models (`RoBERTa`). This approach is capable to perform Q\u0026A across millions of documents in few seconds.","date":"12/10/2020","objectID":"/transformers-building-question-answers-model-at-scale/","tags":["Deep Learning","Transformers","Semantic Search","QA Model","BERT"],"title":"Building Question Answering Model at Scale using ü§óTransformers","uri":"/transformers-building-question-answers-model-at-scale/"},{"categories":["Natural Language Understanding"],"content":"Abstract\r\rIn this article, you will learn how to fetch contextual answers in a huge corpus of documents using Transformersü§ó\r\r ","date":"12/10/2020","objectID":"/transformers-building-question-answers-model-at-scale/:0:0","tags":["Deep Learning","Transformers","Semantic Search","QA Model","BERT"],"title":"Building Question Answering Model at Scale using ü§óTransformers","uri":"/transformers-building-question-answers-model-at-scale/"},{"categories":["Natural Language Understanding"],"content":"Introduction We will build a neural question and answering system using transformers models (RoBERTa). This approach is capable to perform Q\u0026A across millions of documents in few seconds. ","date":"12/10/2020","objectID":"/transformers-building-question-answers-model-at-scale/:1:0","tags":["Deep Learning","Transformers","Semantic Search","QA Model","BERT"],"title":"Building Question Answering Model at Scale using ü§óTransformers","uri":"/transformers-building-question-answers-model-at-scale/"},{"categories":["Natural Language Understanding"],"content":"Data For this tutorial, I will use ArXiV‚Äôs research papers abstracts to do Q\u0026A. The data is on Kaggle. Go to dataset. The dataset has many columns like id author title categories but the columns we will be interested in are title and abstract. abstract contains a long summary of the research paper. We will use this column to build our Question \u0026 Answer model. Let‚Äôs dive into the code. ","date":"12/10/2020","objectID":"/transformers-building-question-answers-model-at-scale/:2:0","tags":["Deep Learning","Transformers","Semantic Search","QA Model","BERT"],"title":"Building Question Answering Model at Scale using ü§óTransformers","uri":"/transformers-building-question-answers-model-at-scale/"},{"categories":["Natural Language Understanding"],"content":"Let‚Äôs Code Note\r\rWe will use Kaggle notebook to write our code so that we can leverage free GPU.\r\r The format of the data is a nested json. We will limit our analysis to just 50,000 documents because of the compute limit on Kaggle to avoid out of memory error. import json data = [] with open(\"/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json\", 'r') as f: for line in f: data.append(json.loads(line)) # Limiting our analysis to 50K documents to avoid memory error data = pd.DataFrame(data[:50000]) # Let's look at the data data.head() We will use abstract column to train our QA model. ","date":"12/10/2020","objectID":"/transformers-building-question-answers-model-at-scale/:3:0","tags":["Deep Learning","Transformers","Semantic Search","QA Model","BERT"],"title":"Building Question Answering Model at Scale using ü§óTransformers","uri":"/transformers-building-question-answers-model-at-scale/"},{"categories":["Natural Language Understanding"],"content":"Haystack Now, Welcome Haystack! The secret sauce behind scaling up to thousands of documents is Haystack. Haystack helps you scale QA models to large collections of documents! You can read more about this amazing library here https://github.com/deepset-ai/haystack For installation: ! pip install git+https://github.com/deepset-ai/haystack.git But just to give a background, there are 3 major components to Haystack. Document Store: Database storing the documents for our search. We recommend Elasticsearch, but have also more light-weight options for fast prototyping (SQL or In-Memory). Retriever: Fast, simple algorithm that identifies candidate passages from a large collection of documents. Algorithms include TF-IDF or BM25, custom Elasticsearch queries, and embedding-based approaches. The Retriever helps to narrow down the scope for Reader to smaller units of text where a given question could be answered. Reader: Powerful neural model that reads through texts in detail to find an answer. Use diverse models like BERT, RoBERTa or XLNet trained via FARM or Transformers on SQuAD like tasks. The Reader takes multiple passages of text as input and returns top-n answers with corresponding confidence scores. You can just load a pretrained model from Hugging Face‚Äôs model hub or fine-tune it to your own domain data. And then there is Finder which glues together a Reader and a Retriever as a pipeline to provide an easy-to-use question answering interface. Now, we can setup Haystack in 3 steps: Install haystack and import its required modules Setup DocumentStore Setup Retriever, Reader and Finder ","date":"12/10/2020","objectID":"/transformers-building-question-answers-model-at-scale/:3:1","tags":["Deep Learning","Transformers","Semantic Search","QA Model","BERT"],"title":"Building Question Answering Model at Scale using ü§óTransformers","uri":"/transformers-building-question-answers-model-at-scale/"},{"categories":["Natural Language Understanding"],"content":"1. Install haystack Let‚Äôs install haystack and import all the required modules # installing haystack ! pip install git+https://github.com/deepset-ai/haystack.git # importing necessary dependencies from haystack import Finder from haystack.indexing.cleaning import clean_wiki_text from haystack.indexing.utils import convert_files_to_dicts, fetch_archive_from_http from haystack.reader.farm import FARMReader from haystack.reader.transformers import TransformersReader from haystack.utils import print_answers ","date":"12/10/2020","objectID":"/transformers-building-question-answers-model-at-scale/:3:2","tags":["Deep Learning","Transformers","Semantic Search","QA Model","BERT"],"title":"Building Question Answering Model at Scale using ü§óTransformers","uri":"/transformers-building-question-answers-model-at-scale/"},{"categories":["Natural Language Understanding"],"content":"2. Setting up DocumentStore Haystack finds answers to queries within the documents stored in a DocumentStore. The current implementations of DocumentStore include ElasticsearchDocumentStore, SQLDocumentStore, and InMemoryDocumentStore. But they recommend ElasticsearchDocumentStore because as it comes preloaded with features like full-text queries, BM25 retrieval, and vector storage for text embeddings. So - Let‚Äôs set up a ElasticsearchDocumentStore. ! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q ! tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz ! chown -R daemon:daemon elasticsearch-7.6.2 import os from subprocess import Popen, PIPE, STDOUT es_server = Popen(['elasticsearch-7.6.2/bin/elasticsearch'], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1) # as daemon ) # wait until ES has started ! sleep 30 # initiating ElasticSearch from haystack.database.elasticsearch import ElasticsearchDocumentStore document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\") Once ElasticsearchDocumentStore is setup, we will write our documents/texts to the DocumentStore. Writing documents to ElasticsearchDocumentStore requires a format - List of dictionaries as shown below: [ {\"name\": \"\u003csome-document-name\u003e, \"text\": \"\u003cthe-actual-text\u003e\"}, {\"name\": \"\u003csome-document-name\u003e, \"text\": \"\u003cthe-actual-text\u003e\"} {\"name\": \"\u003csome-document-name\u003e, \"text\": \"\u003cthe-actual-text\u003e\"} ] (Optionally: you can also add more key-value-pairs here, that will be indexed as fields in Elasticsearch and can be accessed later for filtering or shown in the responses of the Finder) We will use title column to pass as name and abstract column to pass as the text # Now, let's write the dicts containing documents to our DB. document_store.write_documents(data[['title', 'abstract']].rename(columns={'title':'name','abstract':'text'}).to_dict(orient='records')) ","date":"12/10/2020","objectID":"/transformers-building-question-answers-model-at-scale/:3:3","tags":["Deep Learning","Transformers","Semantic Search","QA Model","BERT"],"title":"Building Question Answering Model at Scale using ü§óTransformers","uri":"/transformers-building-question-answers-model-at-scale/"},{"categories":["Natural Language Understanding"],"content":"3. Setup Retriever, Reader and Finder Retrievers help narrowing down the scope for the Reader to smaller units of text where a given question could be answered. They use some simple but fast algorithm. Here: We use Elasticsearch‚Äôs default BM25 algorithm from haystack.retriever.sparse import ElasticsearchRetriever retriever = ElasticsearchRetriever(document_store=document_store) A Reader scans the texts returned by retrievers in detail and extracts the k best answers. They are based on powerful, but slower deep learning models. Haystack currently supports Readers based on the frameworks FARM and Transformers. With both you can either load a local model or one from Hugging Face‚Äôs model hub (https://huggingface.co/models). Here: a medium sized RoBERTa QA model using a Reader based on FARM (https://huggingface.co/deepset/roberta-base-squad2) reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True, context_window_size=500) And finally: The Finder sticks together reader and retriever in a pipeline to answer our actual questions. finder = Finder(reader, retriever) ","date":"12/10/2020","objectID":"/transformers-building-question-answers-model-at-scale/:3:4","tags":["Deep Learning","Transformers","Semantic Search","QA Model","BERT"],"title":"Building Question Answering Model at Scale using ü§óTransformers","uri":"/transformers-building-question-answers-model-at-scale/"},{"categories":["Natural Language Understanding"],"content":"ü•≥ Voila! We‚Äôre Done. Once we have our Finder ready, we are all set to see our model fetching answers for us based on the question. Below is the list of questions that I was asking the model prediction = finder.get_answers(question=\"What do we know about symbiotic stars\", top_k_retriever=10, top_k_reader=2) result = print_answers(prediction, details=\"minimal\") Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.17 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.83 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.71 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.75 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.78 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.83 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01\u003c00:00, 1.08s/ Batches] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01\u003c00:00, 1.09s/ Batches] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.83 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.57 Batches/s] [ { 'answer': 'Their observed population in the\\n' 'Galaxy is however poorly known, and is one to three orders ' 'of magnitudes\\n' 'smaller than the predicted population size', 'context': ' The study of symbiotic stars is essential to understand ' 'important aspects of\\n' 'stellar evolution in interacting binaries. Their observed ' 'population in the\\n' 'Galaxy is however poorly known, and is one to three orders ' 'of magnitudes\\n' 'smaller than the predicted population size. IPHAS, the INT ' 'Photometric Halpha\\n' 'survey of the Northern Galactic plane, gives us the ' 'opportunity to make a\\n' 'systematic, complete search for symbiotic stars in a ' 'magnitude-limited volume,\\n' 'and discover a significant number of new '}, { 'answer': 'Their observed population in the\\n' 'Galaxy is however poorly known, and is one to three orders ' 'of magnitudes\\n' 'smaller than the predicted population size', 'context': ' The study of symbiotic stars is essential to understand ' 'important aspects of\\n' 'stellar evolution in interacting binaries. Their observed ' 'population in the\\n' 'Galaxy is however poorly known, and is one to three orders ' 'of magnitudes\\n' 'smaller than the predicted population size. IPHAS, the INT ' 'Photometric Halpha\\n' 'survey of the Northern Galactic plane, gives us the ' 'opportunity to make a\\n' 'systematic, complete search for symbiotic stars in a ' 'magnitude-limited volume,\\n' 'and discover a significant number of new '}] Let‚Äôs try few more examples - prediction = finder.get_answers(question=\"How is structure of event horizon linked with Morse theory?\", top_k_retriever=10, top_k_reader=2) result = print_answers(prediction, details=\"minimal\") Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.17 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.83 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.71 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.75 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.78 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.83 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01\u003c00:00, 1.08s/ Batches] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01\u003c00:00, 1.09s/ Batches] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.83 Batches/s] Inferencing Samples: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00\u003c00:00, 1.57 Batches/s] [ { 'answer': 'in terms\\nof the Morse theory', 'context': ' The topological structure of the event horizon has been ' 'investigated in terms\\n' 'of the Morse theory. The elementary process of topological ' 'evolution can be\\n' 'understood as a handle attachment. It has been found that ' 'there are certain\\n' 'constraints on the nature of black hole topological ' 'evolution: (i) There are n\\n' 'kinds of handle attachments in (n+1)-dimensional black ' 'hole space-times. (ii)\\n' 'Handles are further classi","date":"12/10/2020","objectID":"/transformers-building-question-answers-model-at-scale/:4:0","tags":["Deep Learning","Transformers","Semantic Search","QA Model","BERT"],"title":"Building Question Answering Model at Scale using ü§óTransformers","uri":"/transformers-building-question-answers-model-at-scale/"},{"categories":["Natural Language Understanding"],"content":"Notebooks Attachments\r\r Go to Dataset Go to Published Kaggle Kernel \r\r","date":"12/10/2020","objectID":"/transformers-building-question-answers-model-at-scale/:5:0","tags":["Deep Learning","Transformers","Semantic Search","QA Model","BERT"],"title":"Building Question Answering Model at Scale using ü§óTransformers","uri":"/transformers-building-question-answers-model-at-scale/"},{"categories":["Text Summarization"],"content":"In this article, you will learn how to train a `T5 model` for text generation - to generate title given a research paper's abstract or summary using Transformersü§ó. For this tutorial, We will take research paper's abstract or brief summary as our input text and its corrosponding paper's title as output text and feed it to a `T5 model` to train. Once the model is trained, it will be able to generate the paper's title based on the abstract. ","date":"11/10/2020","objectID":"/transformers-generating-arxiv-papers-title-from-abstracts/","tags":["Deep Learning","Transformers","T5 Model","Abstractive Summarization"],"title":"Training a T5 Transformer Model - Generating Titles from ArXiv Paper's Abstracts using ü§óTransformers","uri":"/transformers-generating-arxiv-papers-title-from-abstracts/"},{"categories":["Text Summarization"],"content":"Abstract\r\rIn this article, you will learn how to train a T5 model for text generation - to generate title given a research paper‚Äôs abstract or summary using Transformersü§ó\r\r ","date":"11/10/2020","objectID":"/transformers-generating-arxiv-papers-title-from-abstracts/:0:0","tags":["Deep Learning","Transformers","T5 Model","Abstractive Summarization"],"title":"Training a T5 Transformer Model - Generating Titles from ArXiv Paper's Abstracts using ü§óTransformers","uri":"/transformers-generating-arxiv-papers-title-from-abstracts/"},{"categories":["Text Summarization"],"content":"Introduction T5 model is a Sequence-to-Sequence model. A Sequence-to-Sequence model is fully capable to perform any text to text conversion task. What does it mean? - It means that a T5 model can take any input text and convert it into any output text. Such text-to-text conversion is useful in NLP tasks like language translation, summarization, text generation etc. For this tutorial, We will take research paper‚Äôs abstract or brief summary as our input text and its corrosponding paper‚Äôs title as output text and feed it to a T5 model to train. Once the model is trained, it will be able to generate the paper‚Äôs title based on the abstract. So, let‚Äôs dive in. ","date":"11/10/2020","objectID":"/transformers-generating-arxiv-papers-title-from-abstracts/:1:0","tags":["Deep Learning","Transformers","T5 Model","Abstractive Summarization"],"title":"Training a T5 Transformer Model - Generating Titles from ArXiv Paper's Abstracts using ü§óTransformers","uri":"/transformers-generating-arxiv-papers-title-from-abstracts/"},{"categories":["Text Summarization"],"content":"Data ArXiv has recently open-sourced a monstrous dataset of 1.7M research papers on Kaggle. Go to Dataset. We will use its abstract and title columns to train our model. title: This column represents the title of the research paper abstract: This column represents brief summary of the research paper. This will be a supervised training where abstract is our independent variable X while title is our dependent variable y. ","date":"11/10/2020","objectID":"/transformers-generating-arxiv-papers-title-from-abstracts/:2:0","tags":["Deep Learning","Transformers","T5 Model","Abstractive Summarization"],"title":"Training a T5 Transformer Model - Generating Titles from ArXiv Paper's Abstracts using ü§óTransformers","uri":"/transformers-generating-arxiv-papers-title-from-abstracts/"},{"categories":["Text Summarization"],"content":"Let‚Äôs Code Note\r\rWe will use Kaggle notebook to write our code so that we can leverage free GPU.\r\r First, lets install all the dependencies - We will work with latest stable pytorch 1.6. ! pip uninstall torch torchvision -y ! pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html !pip install -U transformers !pip install -U simpletransformers ","date":"11/10/2020","objectID":"/transformers-generating-arxiv-papers-title-from-abstracts/:3:0","tags":["Deep Learning","Transformers","T5 Model","Abstractive Summarization"],"title":"Training a T5 Transformer Model - Generating Titles from ArXiv Paper's Abstracts using ü§óTransformers","uri":"/transformers-generating-arxiv-papers-title-from-abstracts/"},{"categories":["Text Summarization"],"content":"Load the Data The format of the data is a nested json import json data_file = '../input/arxiv/arxiv-metadata-oai-snapshot.json' # Helper function to load the dataset def get_metadata(): with open(data_file, 'r') as f: for line in f: yield line # let's see the first row of the data metadata = get_metadata() for paper in metadata: paper_dict = json.loads(paper) print('Title: {}\\n\\nAbstract: {}\\nRef: {}'.format(paper_dict.get('title'), paper_dict.get('abstract'), paper_dict.get('journal-ref'))) # print(paper) break Title: Calculation of prompt diphoton production cross sections at Tevatron and\rLHC energies Abstract: A fully differential calculation in perturbative quantum chromodynamics is\rpresented for the production of massive photon pairs at hadron colliders. All\rnext-to-leading order perturbative contributions from quark-antiquark,\rgluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\rall-orders resummation of initial-state gluon radiation valid at\rnext-to-next-to-leading logarithmic accuracy. The region of phase space is\rspecified in which the calculation is most reliable. Good agreement is\rdemonstrated with data from the Fermilab Tevatron, and predictions are made for\rmore detailed tests with CDF and DO data. Predictions are shown for\rdistributions of diphoton pairs produced at the energy of the Large Hadron\rCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\rboson are contrasted with those produced from QCD processes at the LHC, showing\rthat enhanced sensitivity to the signal can be obtained with judicious\rselection of events. Ref: Phys.Rev.D76:013009,2007 We have taken 3 attributes of the dataset: Title, Abstract and Ref. Ref is important because the 4 characters of its value give us the year in which the paper was published. We will take last 5 years ArXiv papers (2016-2020) due to Kaggle‚Äôc compute limits titles = [] abstracts = [] years = [] metadata = get_metadata() for paper in metadata: paper_dict = json.loads(paper) ref = paper_dict.get('journal-ref') try: year = int(ref[-4:]) if 2016 \u003c year \u003c 2021: years.append(year) titles.append(paper_dict.get('title')) abstracts.append(paper_dict.get('abstract')) except: pass len(titles), len(abstracts), len(years) (25625, 25625, 25625) So, we have around 25K research papers published from 2016 to 2020. Next, we will convert this data into pandas dataframe and then we will use this data to train our T5 model papers = pd.DataFrame({ 'title': titles, 'abstract': abstracts, 'year': years }) papers.head() ","date":"11/10/2020","objectID":"/transformers-generating-arxiv-papers-title-from-abstracts/:3:1","tags":["Deep Learning","Transformers","T5 Model","Abstractive Summarization"],"title":"Training a T5 Transformer Model - Generating Titles from ArXiv Paper's Abstracts using ü§óTransformers","uri":"/transformers-generating-arxiv-papers-title-from-abstracts/"},{"categories":["Text Summarization"],"content":"T5 Model Training We will use simpletransformers library to train T5 model. This library is based on the Transformers library by HuggingFace. SimpleTransformers lets you quickly train and evaluate Transformer models. Only 3 lines of code are needed to initialize a model, train the model, and evaluate a model. You can read more about it here: https://github.com/ThilinaRajapakse/simpletransformers Input Data Simpletransformers implementation of T5 model expects a data to be a dataframe with 3 columns: \u003cprefix\u003e, \u003cinput_text\u003e, \u003ctarget_text\u003e \u003cprefix\u003e: A string indicating the task to perform. (E.g. ‚Äúquestion‚Äù, ‚Äústsb‚Äù, ‚Äúsummarization‚Äù) \u003cinput_text\u003e: The input text sequence (we will use Paper‚Äôs abstract as input_text ) \u003ctarget_text\u003e: The target sequence (we will use Paper‚Äôs title as output_text ) You can read about the data format: https://github.com/ThilinaRajapakse/simpletransformers#t5-transformer # Adding \u003cinput_text\u003e and \u003ctarget_text\u003e columns papers = papers[['title','abstract']] papers.columns = ['target_text', 'input_text'] # Adding \u003cprefix\u003e columns papers['prefix'] = \"summarize\" # splitting the data into training and test dataset eval_df = papers.sample(frac=0.2, random_state=101) train_df = papers.drop(eval_df.index) train_df.shape, eval_df.shape ((20500, 2), (5125, 2)) We have around 20K research papers for training and 5K papers for evaluation. Setting Training Parameters and Start Training We will train our T5 model with very bare minimum num_train_epochs=4, train_batch_size=16 to fit into Kaggle‚Äôs compute limits. Feel free to play around these training parameters. import logging import pandas as pd from simpletransformers.t5 import T5Model logging.basicConfig(level=logging.INFO) transformers_logger = logging.getLogger(\"transformers\") transformers_logger.setLevel(logging.WARNING) # T5 Training parameters model_args = { \"reprocess_input_data\": True, \"overwrite_output_dir\": True, \"max_seq_length\": 512, \"train_batch_size\": 16, \"num_train_epochs\": 4, } # Create T5 Model model = T5Model(\"t5-small\", args=model_args, use_cuda=True) # Train T5 Model on new task model.train_model(train_df) # Evaluate T5 Model on new task results = model.eval_model(eval_df) print(results) {'eval_loss': 2.103029722170599} It took around 4 hours to train for 4 epochs and with batch_size of 16. And we get a loss of 2.103 on our test data. ","date":"11/10/2020","objectID":"/transformers-generating-arxiv-papers-title-from-abstracts/:3:2","tags":["Deep Learning","Transformers","T5 Model","Abstractive Summarization"],"title":"Training a T5 Transformer Model - Generating Titles from ArXiv Paper's Abstracts using ü§óTransformers","uri":"/transformers-generating-arxiv-papers-title-from-abstracts/"},{"categories":["Text Summarization"],"content":"ü•≥ Voila! We‚Äôre Done Let‚Äôs see how our model performs in generating paper‚Äôs titles Example 1 random_num = 350 actual_title = eval_df.iloc[random_num]['target_text'] actual_abstract = [\"summarize: \"+eval_df.iloc[random_num]['input_text']] predicted_title = model.predict(actual_abstract) print(f'Actual Title: {actual_title}') print(f'Predicted Title: {predicted_title}') print(f'Actual Abstract: {actual_abstract}') Actual Title: Cooperative Passive Coherent Location: A Promising 5G Service to Support\rRoad Safety Predicted Title: ['CPCL: a distributed MIMO radar service for public users'] Actual Abstract: ['summarize: 5G promises many new vertical service areas beyond simple communication and\\ndata transfer. We propose CPCL (cooperative passive coherent location), a\\ndistributed MIMO radar service, which can be offered by mobile radio network\\noperators as a service for public user groups. CPCL comes as an inherent part\\nof the radio network and takes advantage of the most important key features\\nproposed for 5G. It extends the well-known idea of passive radar (also known as\\npassive coherent location, PCL) by introducing cooperative principles. These\\nrange from cooperative, synchronous radio signaling, and MAC up to radar data\\nfusion on sensor and scenario levels. By using software-defined radio and\\nnetwork paradigms, as well as real-time mobile edge computing facilities\\nintended for 5G, CPCL promises to become a ubiquitous radar service which may\\nbe adaptive, reconfigurable, and perhaps cognitive. As CPCL makes double use of\\nradio resources (both in terms of frequency bands and hardware), it can be\\nconsidered a green technology. Although we introduce the CPCL idea from the\\nviewpoint of vehicle-to-vehicle/infrastructure (V2X) communication, it can\\ndefinitely also be applied to many other applications in industry, transport,\\nlogistics, and for safety and security applications.\\n'] Example 2 random_num = 478 actual_title = eval_df.iloc[random_num]['target_text'] actual_abstract = [\"summarize: \"+eval_df.iloc[random_num]['input_text']] predicted_title = model.predict(actual_abstract) print(f'Actual Title: {actual_title}') print(f'Predicted Title: {predicted_title}') print(f'Actual Abstract: {actual_abstract}') Actual Title: Test Model Coverage Analysis under Uncertainty Predicted Title: ['Probabilistic aggregate coverage analysis for model-based testing'] Actual Abstract: ['summarize: In model-based testing (MBT) we may have to deal with a non-deterministic\\nmodel, e.g. because abstraction was applied, or because the software under test\\nitself is non-deterministic. The same test case may then trigger multiple\\npossible execution paths, depending on some internal decisions made by the\\nsoftware. Consequently, performing precise test analyses, e.g. to calculate the\\ntest coverage, are not possible. This can be mitigated if developers can\\nannotate the model with estimated probabilities for taking each transition. A\\nprobabilistic model checking algorithm can subsequently be used to do simple\\nprobabilistic coverage analysis. However, in practice developers often want to\\nknow what the achieved aggregate coverage, which unfortunately cannot be\\nre-expressed as a standard model checking problem. This paper presents an\\nextension to allow efficient calculation of probabilistic aggregate coverage,\\nand moreover also in combination with k-wise coverage.\\n'] Example 3 random_num = 999 actual_title = eval_df.iloc[random_num]['target_text'] actual_abstract = [\"summarize: \"+eval_df.iloc[random_num]['input_text']] predicted_title = model.predict(actual_abstract) print(f'Actual Title: {actual_title}') print(f'Predicted Title: {predicted_title}') print(f'Actual Abstract: {actual_abstract}') Actual Title: Computational intelligence for qualitative coaching diagnostics:\rAutomated assessment of tennis swings to improve performance and safety Predicted Title: ['Personalized qualitative feedback for tennis swing technique using 3D video'] Act","date":"11/10/2020","objectID":"/transformers-generating-arxiv-papers-title-from-abstracts/:4:0","tags":["Deep Learning","Transformers","T5 Model","Abstractive Summarization"],"title":"Training a T5 Transformer Model - Generating Titles from ArXiv Paper's Abstracts using ü§óTransformers","uri":"/transformers-generating-arxiv-papers-title-from-abstracts/"},{"categories":["Text Summarization"],"content":"Notebooks Attachments\r\r Go to Dataset Go to Published Kaggle Kernel \r\r","date":"11/10/2020","objectID":"/transformers-generating-arxiv-papers-title-from-abstracts/:5:0","tags":["Deep Learning","Transformers","T5 Model","Abstractive Summarization"],"title":"Training a T5 Transformer Model - Generating Titles from ArXiv Paper's Abstracts using ü§óTransformers","uri":"/transformers-generating-arxiv-papers-title-from-abstracts/"},{"categories":["Python Packages"],"content":"ResumeAnalyzer is an easy, lightweight python package to rank resumes based on your requirement in just one line of code.","date":"10/10/2020","objectID":"/resume-analyzer/","tags":["Resume Analyzer","Machine Learning","Spacy","Python Packages"],"title":"ResumeAnalyzer | An Easy Solution to Rank Resumes using Spacy","uri":"/resume-analyzer/"},{"categories":["Python Packages"],"content":"Abstract\r\rüöÄ ResumeAnalyzer is an easy, lightweight python package to rank resumes based on your requirement in just one line of code.\r\r ","date":"10/10/2020","objectID":"/resume-analyzer/:0:0","tags":["Resume Analyzer","Machine Learning","Spacy","Python Packages"],"title":"ResumeAnalyzer | An Easy Solution to Rank Resumes using Spacy","uri":"/resume-analyzer/"},{"categories":["Python Packages"],"content":"Introduction Let‚Äôs say, you have an opening for a data scientist in your organization. You post the requirement and you receive thousands of resumes. Great! But there‚Äôs a challenge ‚Äî how do you select the top 50 or 100 most relevant resumes against your requirement description for the next round? If this problem sounds familiar to you, ResumeAnalyzer is here to rescue you. ResumeAnalyzer is an easy, lightweight python package to rank resumes based on your requirement in just one line of code. ","date":"10/10/2020","objectID":"/resume-analyzer/:1:0","tags":["Resume Analyzer","Machine Learning","Spacy","Python Packages"],"title":"ResumeAnalyzer | An Easy Solution to Rank Resumes using Spacy","uri":"/resume-analyzer/"},{"categories":["Python Packages"],"content":"Demo ","date":"10/10/2020","objectID":"/resume-analyzer/:2:0","tags":["Resume Analyzer","Machine Learning","Spacy","Python Packages"],"title":"ResumeAnalyzer | An Easy Solution to Rank Resumes using Spacy","uri":"/resume-analyzer/"},{"categories":["Python Packages"],"content":"How it works? It uses textract to process your documents, spacy‚Äôs PhraseMatcher to rank your resumes and Dash to render UI inside notebook as well as in browser. Here‚Äôs is a complete code looks like ‚Äî # ! pip install ResumeAnalyzer import ResumeAnalyzer as ra analyzer = ra.ResumeAnalyzer() # define the ranking criteria that suits your requirement # E.g. rank candidates based on Deep Learning, Machine Learning and Time Series skills search_criteria = { \"Deep Learning\": [\"neural networks\", \"cnn\", \"rnn\", \"ann\", \"lstm\", \"bert\", \"transformers\"], \"Machine Learning\": [\"regression\", \"classification\", \"clustering\", \"time series\", \"summarization\", \"nlp\"], \"Time Series\": [\"arima\",\"sarimax\", \"prophet\", \"holt winters\"] } # render in jupyter notebook analyzer.render(path=\"Resume Folder/\", metadata=search_criteria, mode=\"notebook\") # render in browser analyzer.render(path=\"Resume Folder/\", metadata=search_criteria, mode=\"browser\") ","date":"10/10/2020","objectID":"/resume-analyzer/:3:0","tags":["Resume Analyzer","Machine Learning","Spacy","Python Packages"],"title":"ResumeAnalyzer | An Easy Solution to Rank Resumes using Spacy","uri":"/resume-analyzer/"},{"categories":["Python Packages"],"content":"Let‚Äôs breakdown the code 1 - Install, Import \u0026 Instantiate ResumeAnalyzer # install ! pip install ResumeAnalyzer # import import ResumeAnalyzer as ra # instantiate analyzer = ra.ResumeAnalyzer() 2 - Define the rank criteria A rank criteria is set of categories and its important terms to rank a candidate‚Äôs resume. # define the ranking criteria that suits your requirement # E.g. rank candidates based on Deep Learning, Machine Learning and Time Series skills search_criteria = { \"Deep Learning\": [\"neural networks\", \"cnn\", \"rnn\", \"ann\", \"lstm\", \"bert\", \"transformers\"], \"Machine Learning\": [\"regression\", \"classification\", \"clustering\", \"time series\", \"summarization\", \"nlp\"], \"Time Series\": [\"arima\",\"sarimax\", \"prophet\", \"holt winters\"] } ResumeAnalyzer passes this information to Spacy‚Äôs PhraseMatcher to calculate if that term is present in resume. If present, +1 is assigned to the resume in that category. For e.g. ‚Äî if ‚ÄúNeural Network‚Äù is present in the resume, the candidates is assigned a +1 in the Deep Learning category. 3 - Render Render renders the results inside jupyter notebook or browser using Dash. # render in jupyter notebook analyzer.render(path=\"Resume Folder/\", metadata=search_criteria, mode=\"notebook\") # render in browser analyzer.render(path=\"Resume Folder/\", metadata=search_criteria, mode=\"browser\") render takes 3 arguments ‚Äî path ‚Äî path to resume folder metadata ‚Äî ranking criteria defined in step 2 mode ‚Äî mode can be ‚Äúnotebook‚Äù if you want to visualize the results in Jupyter Notebook or ‚Äúbrowser‚Äù to see the results in separate browser tab. ","date":"10/10/2020","objectID":"/resume-analyzer/:4:0","tags":["Resume Analyzer","Machine Learning","Spacy","Python Packages"],"title":"ResumeAnalyzer | An Easy Solution to Rank Resumes using Spacy","uri":"/resume-analyzer/"},{"categories":["Python Packages"],"content":"ü•≥Voila ‚Äî We‚Äôre done! The Best Part !‚Äî The above table is filterable as well as sortable. You can sort candidates based on ‚ÄúDeep Learning‚Äù if you want to pickup candidates based on their deep learning skills You can filter candidates with ‚ÄúTotal Score‚Äù \u003e 3 You can also select candidates based on percentile (Ranking): With ‚ÄúRanking‚Äù \u003e0.7 means candidates above 70th percentile. üëã Try it out in Google Colab ","date":"10/10/2020","objectID":"/resume-analyzer/:5:0","tags":["Resume Analyzer","Machine Learning","Spacy","Python Packages"],"title":"ResumeAnalyzer | An Easy Solution to Rank Resumes using Spacy","uri":"/resume-analyzer/"},{"categories":["Python Packages"],"content":"Notebooks Attachments\r\r Go to Github. Go to Google Colab Notebook \r\r","date":"10/10/2020","objectID":"/resume-analyzer/:6:0","tags":["Resume Analyzer","Machine Learning","Spacy","Python Packages"],"title":"ResumeAnalyzer | An Easy Solution to Rank Resumes using Spacy","uri":"/resume-analyzer/"},{"categories":["Text Classification"],"content":"This is the code for downloading and fine tuning pre-trained BERT model on custom dataset for binary text classification","date":"08/10/2020","objectID":"/fine-tune-bert-for-binary-text-classification/","tags":["Deep Learning","Transformers","Text Classification","BERT"],"title":"Codeüìù: Fine Tune BERT Model for Binary Text Classification","uri":"/fine-tune-bert-for-binary-text-classification/"},{"categories":["Text Classification"],"content":" # Requirements: pytorch\u003e=1.6 cudatoolkit=10.2 # install it from here: https://pytorch.org/ # pip install simpletransformers from simpletransformers.classification import ClassificationModel import pandas as pd import logging logging.basicConfig(level=logging.INFO) transformers_logger = logging.getLogger(\"transformers\") transformers_logger.setLevel(logging.WARNING) # Train and Evaluation data needs to be in a Pandas Dataframe of two columns. The first column is the text with type str, and the second column is the label with type int. train_data = [['Example sentence belonging to class 1', 1], ['Example sentence belonging to class 0', 0]] train_df = pd.DataFrame(train_data) eval_data = [['Example eval sentence belonging to class 1', 1], ['Example eval sentence belonging to class 0', 0]] eval_df = pd.DataFrame(eval_data) # Create a ClassificationModel (model_type, model_name): For e.g. # ('roberta', 'roberta-base') # ('albert','albert-base-v2') # ('distilbert', 'distilbert-base-uncased') # Supported model type: CamemBERT, RoBERTa, DistilBERT, ELECTRA, FlauBERT, Longformer, MobileBERT, XLM, XLM-RoBERTa, XLNet model = ClassificationModel('bert', 'bert-base-uncased') # You can set class weights by using the optional weight argument # Train the model model.train_model(train_df) # Evaluate the model result, model_outputs, wrong_predictions = model.eval_model(eval_df) ","date":"08/10/2020","objectID":"/fine-tune-bert-for-binary-text-classification/:0:0","tags":["Deep Learning","Transformers","Text Classification","BERT"],"title":"Codeüìù: Fine Tune BERT Model for Binary Text Classification","uri":"/fine-tune-bert-for-binary-text-classification/"},{"categories":null,"content":"By now, we all are familiar with neural networks and its architecture (input layer, hidden layer, output layer) but one thing that I‚Äôm continuously asked is - ‚Äòwhy do we need activation functions?‚Äô or ‚Äòwhat will happen if we pass the output to the next layer without an activation function‚Äô or ‚ÄòIs nonlinearities really needed by the neural networks?‚Äô","date":"09/09/2020","objectID":"/why-do-we-need-activation-functions/","tags":["Deep Learning","Activation Functions"],"title":"Why Do We Need Activation Functions?","uri":"/why-do-we-need-activation-functions/"},{"categories":null,"content":" By now, we are familiar with neural networks and its architecture (input layer, hidden layer, output layer) but one thing that I‚Äôm continuously asked is - ‚Äúwhy do we need activation functions?\" or ‚Äúwhat will happen if we pass the output to the next layer without an activation function‚Äù or ‚ÄúIs nonlinearities really needed by the neural networks?\" To answer the above questions, let us take a step back and understand what happens inside a neuron: Inside a neuron, each input gets multiplied with the weights $$(x * w) $$ Then, they are summed up $$‚àë(x * w)$$ Then, a bias is added $$‚àë(x * w) + b$$ And then, This output is passed to an activation function. Mathematically, $$y = œÉ (‚àë(x * w) + b)$$ where $œÉ$ is any activation function. An activation function simply defines when a neuron fires. Consider it a sort of tipping point: Input of a certain value won‚Äôt cause the neuron to fire because it‚Äôs not enough, but just a little more input can cause the neuron to fire. In the real-world data, as we model observations using multiple features, each of which could have a varied and disproportional contribution towards determining our output classes. In fact, our world is extremely non-linear, and hence, to capture this non-linearity in our neural network, we need it to incorporate non-linear functions that are capable of representing such phenomena. By doing so, we increase the capacity of our neuron to model more complex patterns that actually exist in the real world, and draw decision boundaries that would not be possible, were we to only use linear functions. These types of functions, used to model non-linear relationships in our data, are known as activation functions. If the neurons don‚Äôt have activation functions, their output would be the weighted sum of the inputs, which is a linear function. Then the entire neural network, that is, a composition of neurons, becomes a composition of linear functions, which is also a linear function. This means that even if we add hidden layers, the network will still be equivalent to a simple linear regression model, with all its limitations. To turn the network into a non-linear function, we‚Äôll use non-linear activation functions for the neurons. Usually, all neurons in the same layer have the same activation function, but different layers may have different activation functions. As with everything else in neural networks, you don‚Äôt have just one activation function. You use the activation function that works best in a particular scenario. With this in mind, you can break the activation functions into these categories: Step, Linear, Sigmoid, Tanh, ReLU, ELU, PReLU, LeakyReLU ","date":"09/09/2020","objectID":"/why-do-we-need-activation-functions/:0:0","tags":["Deep Learning","Activation Functions"],"title":"Why Do We Need Activation Functions?","uri":"/why-do-we-need-activation-functions/"},{"categories":null,"content":"üëã Hey There! My name is Shivanand Roy. I am a huge gear head and spent 4 years to pursue mechanical engineering, but few years ago a serendipitous event inspired me to get into Artificial Intelligence, and I love it. There is nothing greater than solving a business problem through data and predict awesome things, and it‚Äôs a fantastic process that I feel lucky to participate in. Outside of the office, I am committed to education and love to watch PyData, PyCon and Enthought videos. When I‚Äôm not working, I am usually at gym or at swimming pool. I am very much a kid at heart, love to cook, watch cricket, play video games, and travel whenever I can. I love meeting new people and learning new things, so please feel to get in touch. shivanandroy.official@gmail.com Github Medium LinkedIn Twitter ","date":"27/10/2019","objectID":"/contact/:0:0","tags":null,"title":"","uri":"/contact/"},{"categories":null,"content":"üëã Hey There! We will let you know when our next article is up üöÄ #mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; }\r/* Add your own Mailchimp form style overrides in your site stylesheet or in this style block.\rWe recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */\r\rSubscribe\r* indicates required\rFirst Name \r\rLast Name \r\rEmail Address *\r\r\r\r\r \r\r\r\r\r(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';fnames[0]='EMAIL';ftypes[0]='email';fnames[3]='ADDRESS';ftypes[3]='address';fnames[4]='PHONE';ftypes[4]='phone';fnames[5]='BIRTHDAY';ftypes[5]='birthday';}(jQuery));var $mcj = jQuery.noConflict(true);\r","date":"27/10/2019","objectID":"/subscribe/:0:0","tags":null,"title":"","uri":"/subscribe/"},{"categories":null,"content":"Privacy Policy At Shivanandroy.com, accessible from https://Shivanandroy.com/ one of our main priorities is the privacy of our visitors. This Privacy Policy document contains types of information that is collected and recorded by this website and how we use it. If you have additional questions or require more information about our Privacy Policy, do not hesitate to Contact through email at Shivanandroy.official@gmail.com ","date":"01/01/0001","objectID":"/policy/:0:0","tags":null,"title":"","uri":"/policy/"},{"categories":null,"content":"LOG FILES Shivanandroy.com follows a standard procedure of using log files. These files log visitors when they visit websites. All hosting companies do this and a part of hosting services analytics. The information collected by log files include internet protocol (IP) addresses, browser type, Internet Service Provider (ISP), date and time stamp, referring/exit pages, and possibly the number of clicks. These are not linked to any information that is personally identifiable. The purpose of the information is for analyzing trends, administering the site, tracking users' movement on the website, and gathering demographic information. ","date":"01/01/0001","objectID":"/policy/:0:1","tags":null,"title":"","uri":"/policy/"},{"categories":null,"content":"COOKIES AND WEB BEACONS Like any other website, Shivanandroy.com uses ‚Äòcookies‚Äô. These cookies are used to store information including visitors' preferences, and the pages on the website that the visitor accessed or visited. The information is used to optimize the users' experience by customizing our web page content based on visitors' browser type and/or other information. ","date":"01/01/0001","objectID":"/policy/:0:2","tags":null,"title":"","uri":"/policy/"},{"categories":null,"content":"GOOGLE DOUBLECLICK DART COOKIE Google is one of a third-party vendor on our site. It also uses cookies, known as DART cookies, to serve ads to our site visitors based upon their visit to www.website.com and other sites on the internet. However, visitors may choose to decline the use of DART cookies by visiting the Google ad and content network Privacy Policy at the following URL ‚Äì https://policies.google.com/technologies/ads ","date":"01/01/0001","objectID":"/policy/:0:3","tags":null,"title":"","uri":"/policy/"},{"categories":null,"content":"PRIVACY POLICIES You may consult this list to find the Privacy Policy for each of the advertising partners of Shivanandroy.com. Our Privacy Policy was created with the help of the GDPR Privacy Policy Generator and the Privacy Policy Generator from TermsFeed plus the Terms and Conditions Template. Third-party ad servers or ad networks uses technologies like cookies, JavaScript, or Web Beacons that are used in their respective advertisements and links that appear on Shivanandroy.com , which are sent directly to user‚Äôs browser. They automatically receive your IP address when this occurs. These technologies are used to measure the effectiveness of their advertising campaigns and/or to personalize the advertising content that you see on websites that you visit. Note that Shivanandroy.com has no access to or control over these cookies that are used by third-party advertisers. ","date":"01/01/0001","objectID":"/policy/:0:4","tags":null,"title":"","uri":"/policy/"},{"categories":null,"content":"THIRD PARY PRIVACY POLICIES Shivanandroy.com‚Äôs Privacy Policy does not apply to other advertisers or websites. Thus, we are advising you to consult the respective Privacy Policies of these third-party ad servers for more detailed information. It may include their practices and instructions about how to opt-out of certain options. You may find a complete list of these Privacy Policies and their links here: Privacy Policy Links. You can choose to disable cookies through your individual browser options. To know more detailed information about cookie management with specific web browsers, it can be found at the browsers' respective websites. What Are Cookies? ","date":"01/01/0001","objectID":"/policy/:0:5","tags":null,"title":"","uri":"/policy/"},{"categories":null,"content":"CHILDREN‚ÄôS INFORMATION Another part of our priority is adding protection for children while using the internet. We encourage parents and guardians to observe, participate in, and/or monitor and guide their online activity. Shivanandroy.com does not knowingly collect any Personal Identifiable Information from children under the age of 13. If you think that your child provided this kind of information on our website, we strongly encourage you to Contact immediately and we will do our best efforts to promptly remove such information from our records. ","date":"01/01/0001","objectID":"/policy/:0:6","tags":null,"title":"","uri":"/policy/"},{"categories":null,"content":"ONLINE PRIVACY POLICY ONLY This Privacy Policy applies only to our online activities and is valid for visitors to our website with regards to the information that they shared and/or collect in Shivanandroy.com . This policy is not applicable to any information collected offline or via channels other than this website. ","date":"01/01/0001","objectID":"/policy/:0:7","tags":null,"title":"","uri":"/policy/"},{"categories":null,"content":"CONSENT By using our website, you hereby consent to our Privacy Policy and agree to its Terms and Conditions. ","date":"01/01/0001","objectID":"/policy/:0:8","tags":null,"title":"","uri":"/policy/"}]