[{"categories":["Deep Learning"],"content":"Introduction This article details neural question and answering using transformers models (ALBERT) at SCALE. The below approach is capable to perform Q \u0026 A across millions of documents in few seconds. For this tutorial - I will use ArXiV‚Äôs research papers abstracts to do Q\u0026A. The data is uploaded on Kaggle here.. Now let‚Äôs dive in‚Ä¶ Reading the entire json metadata and then limiting my analysis to just 50,000 documents because of the compute limit on Kaggle. import json data = [] with open(\"/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json\", 'r') as f: for line in f: data.append(json.loads(line)) data = pd.DataFrame(data[:50000]) # Let's look at the data data.head() We will use abstract column to train our QA model. Now, Welcome Haystack! The secret sauce behind scaling up is Haystack. It lets you scale QA models to large collections of documents! You can read more about this amazing library here https://github.com/deepset-ai/haystack For installation: ! pip install git+https://github.com/deepset-ai/haystack.git But just to give a background, there are 3 major components to Haystack. Document Store: Database storing the documents for our search. We recommend Elasticsearch, but have also more light-weight options for fast prototyping (SQL or In-Memory). Retriever: Fast, simple algorithm that identifies candidate passages from a large collection of documents. Algorithms include TF-IDF or BM25, custom Elasticsearch queries, and embedding-based approaches. The Retriever helps to narrow down the scope for Reader to smaller units of text where a given question could be answered. Reader: Powerful neural model that reads through texts in detail to find an answer. Use diverse models like BERT, RoBERTa or XLNet trained via FARM or Transformers on SQuAD like tasks. The Reader takes multiple passages of text as input and returns top-n answers with corresponding confidence scores. You can just load a pretrained model from Hugging Face‚Äôs model hub or fine-tune it to your own domain data. And then there is Finder which glues together a Reader and a Retriever as a pipeline to provide an easy-to-use question answering interface. # installing haystack ! pip install git+https://github.com/deepset-ai/haystack.git # importing necessary dependencies from haystack import Finder from haystack.indexing.cleaning import clean_wiki_text from haystack.indexing.utils import convert_files_to_dicts, fetch_archive_from_http from haystack.reader.farm import FARMReader from haystack.reader.transformers import TransformersReader from haystack.utils import print_answers Setting up DocumentStore Haystack finds answers to queries within the documents stored in a DocumentStore. The current implementations of DocumentStore include ElasticsearchDocumentStore, SQLDocumentStore, and InMemoryDocumentStore. But they recommend ElasticsearchDocumentStore because as it comes preloaded with features like full-text queries, BM25 retrieval, and vector storage for text embeddings. So - Let‚Äôs set up a ElasticsearchDocumentStore. ! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q ! tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz ! chown -R daemon:daemon elasticsearch-7.6.2 import os from subprocess import Popen, PIPE, STDOUT es_server = Popen(['elasticsearch-7.6.2/bin/elasticsearch'], stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1) # as daemon ) # wait until ES has started ! sleep 30 # initiating ElasticSearch from haystack.database.elasticsearch import ElasticsearchDocumentStore document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\") Once ElasticsearchDocumentStore is setup, we will write our documents/texts to the DocumentStore. Writing documents to ElasticsearchDocumentStore requires a format - List of dictionaries The default format here is: [{\"name\": \"\u003csome-document-name\u003e, \"text\": \"\u003cthe-actual-text\u003e\"}, {\"name\": \"\u003csome-document-name\u003e, \"text\": \"\u003cthe-actual-text\u003e\"","date":"2020-08-19","objectID":"/transformers-building-question-answers-model-at-scale/:0:1","tags":["Deep Learning","Transformers","Question \u0026 Answering"],"title":"ü§óTransformers: Building Question \u0026 Answering Model at Scale","uri":"/transformers-building-question-answers-model-at-scale/"},{"categories":["Deep Learning"],"content":"In this article, we will see how to use a T5 model to generate research paper‚Äôs title based on paper‚Äôs abstracts. T5 model is seq-to-seq model i.e. A Sequence to Sequence model fully capable to perform any text to text tasks. What does it mean - It means that T5 model can take any input text and convert it into any output text. Such text-to-text conversion is useful in NLP tasks like language translation, summarization etc. We will take paper‚Äôs abstracts as our input text and paper‚Äôs title as output text and feed it to T5 model. Once the model is trained, it will be able to generate the paper‚Äôs title based on the abstract. So, let‚Äôs dive in. ","date":"2020-08-19","objectID":"/transformers-generating-arxiv-papers-title-from-abstracts/:0:0","tags":["Deep Learning","Transformers","T5 Model","Summarization"],"title":"ü§óTransformers: Training a T5 Transformer Model - Generating ArXiv Paper's Titles from Abstracts","uri":"/transformers-generating-arxiv-papers-title-from-abstracts/"},{"categories":["Deep Learning"],"content":"Dataset ArXiv has recently open-sourced a monstrous dataset of 1.7M research papers on Kaggle. We will use its abstract and title columns to train our model. title: This column represents the title of the research paper abstract: This column represents brief summary of the research paper. This will be a supervised training where abstract is our independent variable (X) while title is our dependent variable (y). ","date":"2020-08-19","objectID":"/transformers-generating-arxiv-papers-title-from-abstracts/:0:1","tags":["Deep Learning","Transformers","T5 Model","Summarization"],"title":"ü§óTransformers: Training a T5 Transformer Model - Generating ArXiv Paper's Titles from Abstracts","uri":"/transformers-generating-arxiv-papers-title-from-abstracts/"},{"categories":["Deep Learning"],"content":"Code We will install dependencies and work with latest stable pytorch 1.6 ! pip uninstall torch torchvision -y ! pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html !pip install -U transformers !pip install -U simpletransformers let‚Äôs load the data import json data_file = '../input/arxiv/arxiv-metadata-oai-snapshot.json' def get_metadata(): with open(data_file, 'r') as f: for line in f: yield line metadata = get_metadata() for paper in metadata: paper_dict = json.loads(paper) print('Title: {}\\n\\nAbstract: {}\\nRef: {}'.format(paper_dict.get('title'), paper_dict.get('abstract'), paper_dict.get('journal-ref'))) # print(paper) break Title: Calculation of prompt diphoton production cross sections at Tevatron and LHC energies Abstract: A fully differential calculation in perturbative quantum chromodynamics is presented for the production of massive photon pairs at hadron colliders. All next-to-leading order perturbative contributions from quark-antiquark, gluon-(anti)quark, and gluon-gluon subprocesses are included, as well as all-orders resummation of initial-state gluon radiation valid at next-to-next-to-leading logarithmic accuracy. The region of phase space is specified in which the calculation is most reliable. Good agreement is demonstrated with data from the Fermilab Tevatron, and predictions are made for more detailed tests with CDF and DO data. Predictions are shown for distributions of diphoton pairs produced at the energy of the Large Hadron Collider (LHC). Distributions of the diphoton pairs from the decay of a Higgs boson are contrasted with those produced from QCD processes at the LHC, showing that enhanced sensitivity to the signal can be obtained with judicious selection of events. Ref: Phys.Rev.D76:013009,2007 We will take last 5 years ArXiv papers (2016-2021) due to Kaggle‚Äôc compute limits titles = [] abstracts = [] years = [] metadata = get_metadata() for paper in metadata: paper_dict = json.loads(paper) ref = paper_dict.get('journal-ref') try: year = int(ref[-4:]) if 2016 \u003c year \u003c 2021: years.append(year) titles.append(paper_dict.get('title')) abstracts.append(paper_dict.get('abstract')) except: pass len(titles), len(abstracts), len(years) (25625, 25625, 25625) papers = pd.DataFrame({ 'title': titles, 'abstract': abstracts, 'year': years }) papers.head() We will use simpletransformers library to train a T5 model Simpletransformers implementation of T5 model expects a data to be a dataframe with 3 columns: \u003cprefix\u003e, \u003cinput_text\u003e, \u003ctarget_text\u003e \u003cprefix\u003e: A string indicating the task to perform. (E.g. ‚Äúquestion‚Äù, ‚Äústsb‚Äù) \u003cinput_text\u003e: The input text sequence (we will use Paper‚Äôs abstract as input_text ) \u003ctarget_text\u003e: The target sequence (we will use Paper‚Äôs title as output_text ) You can read about the data format: https://github.com/ThilinaRajapakse/simpletransformers#t5-transformer papers = papers[['title','abstract']] papers.columns = ['target_text', 'input_text'] papers = papers.dropna() # splitting the data into training and test dataset eval_df = papers.sample(frac=0.2, random_state=101) train_df = papers.drop(eval_df.index) train_df.shape, eval_df.shape ((20500, 2), (5125, 2)) We will training our T5 model with very bare minimum num_train_epochs=4, train_batch_size=16 to fit into Kaggle‚Äôs compute limits import logging import pandas as pd from simpletransformers.t5 import T5Model logging.basicConfig(level=logging.INFO) transformers_logger = logging.getLogger(\"transformers\") transformers_logger.setLevel(logging.WARNING) train_df['prefix'] = \"summarize\" eval_df['prefix'] = \"summarize\" model_args = { \"reprocess_input_data\": True, \"overwrite_output_dir\": True, \"max_seq_length\": 512, \"train_batch_size\": 16, \"num_train_epochs\": 4, } # Create T5 Model model = T5Model(\"t5-small\", args=model_args, use_cuda=True) # Train T5 Model on new task model.train_model(train_df) # Evaluate T5 Model on new task results = model.eval_model(eval_df) print(results) {'eval_loss","date":"2020-08-19","objectID":"/transformers-generating-arxiv-papers-title-from-abstracts/:0:2","tags":["Deep Learning","Transformers","T5 Model","Summarization"],"title":"ü§óTransformers: Training a T5 Transformer Model - Generating ArXiv Paper's Titles from Abstracts","uri":"/transformers-generating-arxiv-papers-title-from-abstracts/"}]