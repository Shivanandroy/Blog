<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Shivanand Roy | Deep Learning</title>
        <link>http://shivanandroy.com/</link>
        <description>This blog presents articles on machine learning and deep learning. The articles covers natural language processing, natural language generation, classification, regression, text analytics, nlp, pretrained nlp models, transformers based models.</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>iamshivanandroy@gmail.com (Shivanand Roy)</managingEditor>
            <webMaster>iamshivanandroy@gmail.com (Shivanand Roy)</webMaster><lastBuildDate>Mon, 08 Feb 2021 00:42:27 &#43;0530</lastBuildDate>
            <atom:link href="http://shivanandroy.com/index.xml" rel="self" type="application/rss+xml" />
        <item>
    <title>Fine Tuning T5 Transformer Model with PyTorch</title>
    <link>http://shivanandroy.com/fine-tune-t5-transformer-with-pytorch/</link>
    <pubDate>Mon, 08 Feb 2021 00:42:27 &#43;0530</pubDate>
    <author>Author</author>
    <guid>http://shivanandroy.com/fine-tune-t5-transformer-with-pytorch/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/dl/images/fine-tune-t5.png" referrerpolicy="no-referrer">
            </div>In this article, you will learn how to fine tune a T5 model with PyTorch and transformers]]></description>
</item><item>
    <title>Visualizing Game of Thrones with BERT</title>
    <link>http://shivanandroy.com/visualizing-game-of-thrones-with-bert/</link>
    <pubDate>Fri, 13 Nov 2020 00:42:27 &#43;0530</pubDate>
    <author>Author</author>
    <guid>http://shivanandroy.com/visualizing-game-of-thrones-with-bert/</guid>
    <description><![CDATA[In this article, we will visualize Game of Thrones books with BERT in 3D space.]]></description>
</item><item>
    <title>NLP360 : Awesome NLP Resources</title>
    <link>http://shivanandroy.com/awesome-nlp-resources/</link>
    <pubDate>Sat, 31 Oct 2020 00:44:27 &#43;0530</pubDate>
    <author>Author</author>
    <guid>http://shivanandroy.com/awesome-nlp-resources/</guid>
    <description><![CDATA[NLP360 is curated list of resources related to Natural Language Processing (NLP) x updated weekly]]></description>
</item><item>
    <title>Orbit | Uber&#39;s New Open Source Python Library for Time Series Modeling</title>
    <link>http://shivanandroy.com/orbit-uber-opensources-python-package-for-time-series-modeling/</link>
    <pubDate>Fri, 30 Oct 2020 00:42:27 &#43;0530</pubDate>
    <author>Author</author>
    <guid>http://shivanandroy.com/orbit-uber-opensources-python-package-for-time-series-modeling/</guid>
    <description><![CDATA[Uber just open-sourced its time series modeling packagae - Orbit based on probabilistic modeling]]></description>
</item><item>
    <title>Top 10 Most Useful But Underrated Python Libraries for DataÂ Science</title>
    <link>http://shivanandroy.com/top-useful-but-underrated-python-libraries-for-data-science/</link>
    <pubDate>Sun, 18 Oct 2020 00:42:27 &#43;0530</pubDate>
    <author>Author</author>
    <guid>http://shivanandroy.com/top-useful-but-underrated-python-libraries-for-data-science/</guid>
    <description><![CDATA[Top 10 most useful but underrated python libraries for data science and machine learning]]></description>
</item><item>
    <title>Building A Faster &amp; Accurate COVID Search Engine with TransformersðŸ¤—</title>
    <link>http://shivanandroy.com/building-a-faster-accurate-covid-search-engine-with-transformers/</link>
    <pubDate>Wed, 14 Oct 2020 00:42:27 &#43;0530</pubDate>
    <author>Author</author>
    <guid>http://shivanandroy.com/building-a-faster-accurate-covid-search-engine-with-transformers/</guid>
    <description><![CDATA[This article is a step by step guide to build a faster and accurate COVID Semantic Search Engine using HuggingFace TransformersðŸ¤—. In this article, we will build a search engine, which will not only retrieve and rank the articles based on the query but also give us the response, along with a 1000 words context around the response]]></description>
</item><item>
    <title>Fine Tuning XLNet Model for Text Classification</title>
    <link>http://shivanandroy.com/fine-tuning-xlnet-model-for-text-classification/</link>
    <pubDate>Tue, 13 Oct 2020 00:40:27 &#43;0530</pubDate>
    <author>Author</author>
    <guid>http://shivanandroy.com/fine-tuning-xlnet-model-for-text-classification/</guid>
    <description><![CDATA[In this article, we will see how to fine tune a XLNet model on custom data, for text classification using TransformersðŸ¤—. XLNet is powerful! It beats BERT and its other variants in 20 different tasks. In simple words - XLNet is a generalized autoregressive model.
An Autoregressive model is a model which uses the context word to predict the next word. So, the next token is dependent on all previous tokens.
XLNET is generalized because it captures bi-directional context by means of a mechanism called permutation language modeling.
It integrates the idea of auto-regressive models and bi-directional context modeling, yet overcoming the disadvantages of BERT and thus outperforming BERT on 20 tasks, often by a large margin in tasks such as question answering, natural language inference, sentiment analysis, and document ranking.

In this article, we will take a pretrained `XLNet` model and fine tune it on our dataset.]]></description>
</item><item>
    <title>Building Question Answering Model at Scale using ðŸ¤—Transformers</title>
    <link>http://shivanandroy.com/transformers-building-question-answers-model-at-scale/</link>
    <pubDate>Mon, 12 Oct 2020 00:42:27 &#43;0530</pubDate>
    <author>Author</author>
    <guid>http://shivanandroy.com/transformers-building-question-answers-model-at-scale/</guid>
    <description><![CDATA[In this article, you will learn how to fetch contextual answers in a huge corpus of documents using TransformersðŸ¤—. We will build a neural question and answering system using transformers models (`RoBERTa`). This approach is capable to perform Q&A across millions of documents in few seconds.]]></description>
</item><item>
    <title>Training a T5 Transformer Model - Generating Titles from ArXiv Paper&#39;s Abstracts using ðŸ¤—Transformers</title>
    <link>http://shivanandroy.com/transformers-generating-arxiv-papers-title-from-abstracts/</link>
    <pubDate>Sun, 11 Oct 2020 00:40:27 &#43;0530</pubDate>
    <author>Author</author>
    <guid>http://shivanandroy.com/transformers-generating-arxiv-papers-title-from-abstracts/</guid>
    <description><![CDATA[In this article, you will learn how to train a `T5 model` for text generation - to generate title given a research paper's abstract or summary using TransformersðŸ¤—. For this tutorial, We will take research paper's abstract or brief summary as our input text and its corrosponding paper's title as output text and feed it to a `T5 model` to train. Once the model is trained, it will be able to generate the paper's title based on the abstract. ]]></description>
</item><item>
    <title>ResumeAnalyzer | An Easy Solution to Rank Resumes using Spacy</title>
    <link>http://shivanandroy.com/resume-analyzer/</link>
    <pubDate>Sat, 10 Oct 2020 00:42:27 &#43;0530</pubDate>
    <author>Author</author>
    <guid>http://shivanandroy.com/resume-analyzer/</guid>
    <description><![CDATA[ResumeAnalyzer is an easy, lightweight python package to rank resumes based on your requirement in just one line of code.]]></description>
</item></channel>
</rss>
