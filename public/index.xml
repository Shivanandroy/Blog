<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Shivanand Roy | Deep Learning</title>
        <link>http://shivanandroy.com/</link>
        <description>Shivanand Roy | Deep Learning</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>Shivanandroy.official@gmail.com (Shivanand Roy)</managingEditor>
            <webMaster>Shivanandroy.official@gmail.com (Shivanand Roy)</webMaster><lastBuildDate>Tue, 01 Sep 2020 00:40:27 &#43;0530</lastBuildDate>
            <atom:link href="http://shivanandroy.com/index.xml" rel="self" type="application/rss+xml" />
        <item>
    <title>Training a T5 Transformer Model - Generating Titles from ArXiv Paper&#39;s Abstracts using ðŸ¤—Transformers</title>
    <link>http://shivanandroy.com/transformers-generating-arxiv-papers-title-from-abstracts/</link>
    <pubDate>Tue, 01 Sep 2020 00:40:27 &#43;0530</pubDate>
    <author>Author</author>
    <guid>http://shivanandroy.com/transformers-generating-arxiv-papers-title-from-abstracts/</guid>
    <description><![CDATA[Abstract  In this article, you will learn how to train a T5 model for text generation - to generate title given a research paper&rsquo;s abstract or summary using TransformersðŸ¤—   Introduction T5 model is a Sequence-to-Sequence model. A Sequence-to-Sequence model is fully capable to perform any text to text conversion task. What does it mean? - It means that a T5 model can take any input text and convert it into any output text.]]></description>
</item><item>
    <title>Building Question Answering Model at Scale using ðŸ¤—Transformers</title>
    <link>http://shivanandroy.com/transformers-building-question-answers-model-at-scale/</link>
    <pubDate>Wed, 19 Aug 2020 00:42:27 &#43;0530</pubDate>
    <author>Author</author>
    <guid>http://shivanandroy.com/transformers-building-question-answers-model-at-scale/</guid>
    <description><![CDATA[Abstract  In this article, you will learn how to fetch contextual answers in a huge corpus of documents using TransformersðŸ¤—     Introduction We will build a neural question and answering system using transformers models (RoBERTa). This approach is capable to perform Q&amp;A across millions of documents in few seconds.
Data For this tutorial, I will use ArXiV&rsquo;s research papers abstracts to do Q&amp;A. The data is on Kaggle.]]></description>
</item></channel>
</rss>
