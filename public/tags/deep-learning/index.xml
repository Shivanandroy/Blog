<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Deep Learning - Tag - Shivanand Roy | Deep Learning</title>
        <link>http://shivanandroy.com/tags/deep-learning/</link>
        <description>Deep Learning - Tag - Shivanand Roy | Deep Learning</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>iamshivanandroy@gmail.com (Shivanand Roy)</managingEditor>
            <webMaster>iamshivanandroy@gmail.com (Shivanand Roy)</webMaster><lastBuildDate>Mon, 08 Feb 2021 00:42:27 &#43;0530</lastBuildDate><atom:link href="http://shivanandroy.com/tags/deep-learning/" rel="self" type="application/rss+xml" /><item>
    <title>Fine Tuning T5 Transformer Model with PyTorch</title>
    <link>http://shivanandroy.com/fine-tune-t5-transformer-with-pytorch/</link>
    <pubDate>Mon, 08 Feb 2021 00:42:27 &#43;0530</pubDate>
    <author>Author</author>
    <guid>http://shivanandroy.com/fine-tune-t5-transformer-with-pytorch/</guid>
    <description><![CDATA[<div class="featured-image">
                <img src="/posts/dl/images/fine-tune-t5.png" referrerpolicy="no-referrer">
            </div>In this article, you will learn how to fine tune a T5 model with PyTorch and transformers]]></description>
</item><item>
    <title>Visualizing Game of Thrones with BERT</title>
    <link>http://shivanandroy.com/visualizing-game-of-thrones-with-bert/</link>
    <pubDate>Fri, 13 Nov 2020 00:42:27 &#43;0530</pubDate>
    <author>Author</author>
    <guid>http://shivanandroy.com/visualizing-game-of-thrones-with-bert/</guid>
    <description><![CDATA[In this article, we will visualize Game of Thrones books with BERT in 3D space.]]></description>
</item><item>
    <title>Building A Faster &amp; Accurate COVID Search Engine with Transformersü§ó</title>
    <link>http://shivanandroy.com/building-a-faster-accurate-covid-search-engine-with-transformers/</link>
    <pubDate>Wed, 14 Oct 2020 00:42:27 &#43;0530</pubDate>
    <author>Author</author>
    <guid>http://shivanandroy.com/building-a-faster-accurate-covid-search-engine-with-transformers/</guid>
    <description><![CDATA[This article is a step by step guide to build a faster and accurate COVID Semantic Search Engine using HuggingFace Transformersü§ó. In this article, we will build a search engine, which will not only retrieve and rank the articles based on the query but also give us the response, along with a 1000 words context around the response]]></description>
</item><item>
    <title>Fine Tuning XLNet Model for Text Classification</title>
    <link>http://shivanandroy.com/fine-tuning-xlnet-model-for-text-classification/</link>
    <pubDate>Tue, 13 Oct 2020 00:40:27 &#43;0530</pubDate>
    <author>Author</author>
    <guid>http://shivanandroy.com/fine-tuning-xlnet-model-for-text-classification/</guid>
    <description><![CDATA[In this article, we will see how to fine tune a XLNet model on custom data, for text classification using Transformersü§ó. XLNet is powerful! It beats BERT and its other variants in 20 different tasks. In simple words - XLNet is a generalized autoregressive model.
An Autoregressive model is a model which uses the context word to predict the next word. So, the next token is dependent on all previous tokens.
XLNET is generalized because it captures bi-directional context by means of a mechanism called permutation language modeling.
It integrates the idea of auto-regressive models and bi-directional context modeling, yet overcoming the disadvantages of BERT and thus outperforming BERT on 20 tasks, often by a large margin in tasks such as question answering, natural language inference, sentiment analysis, and document ranking.

In this article, we will take a pretrained `XLNet` model and fine tune it on our dataset.]]></description>
</item><item>
    <title>Building Question Answering Model at Scale using ü§óTransformers</title>
    <link>http://shivanandroy.com/transformers-building-question-answers-model-at-scale/</link>
    <pubDate>Mon, 12 Oct 2020 00:42:27 &#43;0530</pubDate>
    <author>Author</author>
    <guid>http://shivanandroy.com/transformers-building-question-answers-model-at-scale/</guid>
    <description><![CDATA[In this article, you will learn how to fetch contextual answers in a huge corpus of documents using Transformersü§ó. We will build a neural question and answering system using transformers models (`RoBERTa`). This approach is capable to perform Q&A across millions of documents in few seconds.]]></description>
</item><item>
    <title>Training a T5 Transformer Model - Generating Titles from ArXiv Paper&#39;s Abstracts using ü§óTransformers</title>
    <link>http://shivanandroy.com/transformers-generating-arxiv-papers-title-from-abstracts/</link>
    <pubDate>Sun, 11 Oct 2020 00:40:27 &#43;0530</pubDate>
    <author>Author</author>
    <guid>http://shivanandroy.com/transformers-generating-arxiv-papers-title-from-abstracts/</guid>
    <description><![CDATA[In this article, you will learn how to train a `T5 model` for text generation - to generate title given a research paper's abstract or summary using Transformersü§ó. For this tutorial, We will take research paper's abstract or brief summary as our input text and its corrosponding paper's title as output text and feed it to a `T5 model` to train. Once the model is trained, it will be able to generate the paper's title based on the abstract. ]]></description>
</item><item>
    <title>Codeüìù: Fine Tune BERT Model for Binary Text Classification</title>
    <link>http://shivanandroy.com/fine-tune-bert-for-binary-text-classification/</link>
    <pubDate>Thu, 08 Oct 2020 00:42:27 &#43;0530</pubDate>
    <author>Author</author>
    <guid>http://shivanandroy.com/fine-tune-bert-for-binary-text-classification/</guid>
    <description><![CDATA[This is the code for downloading and fine tuning pre-trained BERT model on custom dataset for binary text classification]]></description>
</item><item>
    <title>Why Do We Need Activation Functions?</title>
    <link>http://shivanandroy.com/why-do-we-need-activation-functions/</link>
    <pubDate>Wed, 09 Sep 2020 00:42:27 &#43;0530</pubDate>
    <author>Author</author>
    <guid>http://shivanandroy.com/why-do-we-need-activation-functions/</guid>
    <description><![CDATA[By now, we all are familiar with neural networks and its architecture (input layer, hidden layer, output layer) but one thing that I‚Äôm continuously asked is - ‚Äòwhy do we need activation functions?‚Äô or ‚Äòwhat will happen if we pass the output to the next layer without an activation function‚Äô or ‚ÄòIs nonlinearities really needed by the neural networks?‚Äô]]></description>
</item></channel>
</rss>
